apiVersion: apps/v1
kind: Deployment
metadata:
  name: localai
  namespace: ai
spec:
  replicas: 1
  selector:
    matchLabels:
      app: localai
  template:
    metadata:
      labels:
        app: localai
    spec:
      nodeSelector:
        gpu: "true"
      tolerations:
        - key: "node-role.kubernetes.io/control-plane"
          operator: "Exists"
          effect: "NoSchedule"
      containers:
        - name: localai
          image: ghcr.io/mudler/localai
          ports:
            - containerPort: 8080
          resources:
            limits:
              nvidia.com/gpu: 1
          volumeMounts:
            - name: models
              mountPath: /root/.localai/models
          env:
              value: "/root/.localai/models"
            # optional: enable CUDA
            - name: LOCALAI_OPENCL
              value: "false"
            - name: LOCALAI_CUDA
              value: "true"
      volumes:
        - name: models
          persistentVolumeClaim:
            claimName: localai-models
