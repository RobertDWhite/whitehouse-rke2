apiVersion: batch/v1
kind: Job
metadata:
  name: localai-image-model-bootstrap
  namespace: ai-stack
  annotations:
    argocd.argoproj.io/hook: PostSync
    argocd.argoproj.io/hook-delete-policy: BeforeHookCreation,HookSucceeded
spec:
  backoffLimit: 2
  ttlSecondsAfterFinished: 300
  template:
    spec:
      nodeSelector:
        gpu: "true"
      tolerations:
        - key: "node-role.kubernetes.io/control-plane"
          operator: "Exists"
          effect: "NoSchedule"
      restartPolicy: OnFailure
      containers:
        - name: bootstrap
          image: python:3.12-alpine
          env:
            - name: LOCALAI_BASE_URL
              value: "http://localai.ai-stack.svc.cluster.local:8080"
            - name: LOCALAI_IMAGE_MODEL_NAME
              value: "dreamshaper"
            - name: LOCALAI_IMAGE_BACKEND_NAME
              value: "diffusers"
            - name: LOCALAI_IMAGE_MODEL_GALLERY_URL
              value: "github:mudler/LocalAI/gallery/dreamshaper.yaml@master"
            - name: LOCALAI_BOOTSTRAP_TIMEOUT_SECONDS
              value: "7200"
            - name: LOCALAI_BOOTSTRAP_POLL_SECONDS
              value: "15"
            - name: LOCALAI_MAX_INSTALL_ATTEMPTS
              value: "4"
            - name: LOCALAI_HTTP_MAX_ATTEMPTS
              value: "6"
            - name: LOCALAI_HTTP_TIMEOUT_SECONDS
              value: "90"
            - name: LOCALAI_PREFETCH_ATTEMPTS
              value: "8"
            - name: LOCALAI_IMAGE_MODEL_FILE_URL
              value: "https://huggingface.co/Lykon/DreamShaper/resolve/main/DreamShaper_8_pruned.safetensors"
            - name: LOCALAI_IMAGE_MODEL_FILE_NAME
              value: "DreamShaper_8_pruned.safetensors"
            - name: LOCALAI_MODEL_URLS_FILE
              value: "/config/huggingface-urls.txt"
            - name: LOCALAI_MODEL_REPOS_FILE
              value: "/config/huggingface-repos.txt"
            - name: HF_TOKEN
              valueFrom:
                secretKeyRef:
                  name: localai-model-downloads
                  key: hf_token
                  optional: true
          volumeMounts:
            - name: models
              mountPath: /models
            - name: model-urls
              mountPath: /config
              readOnly: true
          command:
            - /bin/sh
            - -lc
          args:
            - |
              python - <<'PY'
              import json
              import os
              import subprocess
              import sys
              import time
              import urllib.error
              import urllib.parse
              import urllib.request

              base_url = os.getenv("LOCALAI_BASE_URL", "http://localai:8080").rstrip("/")
              model_name = os.getenv("LOCALAI_IMAGE_MODEL_NAME", "dreamshaper")
              backend_name = os.getenv("LOCALAI_IMAGE_BACKEND_NAME", "diffusers")
              model_gallery_url = os.getenv(
                  "LOCALAI_IMAGE_MODEL_GALLERY_URL",
                  "github:mudler/LocalAI/gallery/dreamshaper.yaml@master",
              )
              timeout_seconds = int(os.getenv("LOCALAI_BOOTSTRAP_TIMEOUT_SECONDS", "7200"))
              poll_seconds = int(os.getenv("LOCALAI_BOOTSTRAP_POLL_SECONDS", "15"))
              max_install_attempts = int(os.getenv("LOCALAI_MAX_INSTALL_ATTEMPTS", "4"))
              max_http_attempts = int(os.getenv("LOCALAI_HTTP_MAX_ATTEMPTS", "6"))
              http_timeout_seconds = int(os.getenv("LOCALAI_HTTP_TIMEOUT_SECONDS", "90"))
              prefetch_attempts = int(os.getenv("LOCALAI_PREFETCH_ATTEMPTS", "8"))
              prefetch_file_url = os.getenv("LOCALAI_IMAGE_MODEL_FILE_URL", "").strip()
              prefetch_file_name = os.getenv("LOCALAI_IMAGE_MODEL_FILE_NAME", "").strip()
              model_urls_file = os.getenv("LOCALAI_MODEL_URLS_FILE", "/config/huggingface-urls.txt").strip()
              model_repos_file = os.getenv("LOCALAI_MODEL_REPOS_FILE", "/config/huggingface-repos.txt").strip()
              hf_token = os.getenv("HF_TOKEN", "").strip()

              def http_request(method, path, payload=None):
                  data = None
                  headers = {}
                  if payload is not None:
                      data = json.dumps(payload).encode("utf-8")
                      headers["Content-Type"] = "application/json"
                  req = urllib.request.Request(
                      f"{base_url}{path}",
                      data=data,
                      method=method,
                      headers=headers,
                  )
                  last_exc = None
                  for attempt in range(1, max_http_attempts + 1):
                      try:
                          with urllib.request.urlopen(req, timeout=http_timeout_seconds) as resp:
                              body = resp.read().decode("utf-8")
                          if not body:
                              return {}
                          return json.loads(body)
                      except Exception as exc:
                          last_exc = exc
                          if attempt < max_http_attempts:
                              print(
                                  f"http retry {attempt}/{max_http_attempts} for {method} {path}: {exc}",
                                  flush=True,
                              )
                              time.sleep(5)
                              continue
                  raise last_exc

              def get_installed_models():
                  response = http_request("GET", "/v1/models")
                  data = response.get("data", [])
                  names = set()
                  for item in data:
                      if isinstance(item, dict):
                          if item.get("id"):
                              names.add(str(item["id"]))
                          if item.get("name"):
                              names.add(str(item["name"]))
                  return names

              def get_installed_backends():
                  response = http_request("GET", "/backends")
                  names = set()
                  if isinstance(response, list):
                      for item in response:
                          if isinstance(item, str):
                              names.add(item)
                          elif isinstance(item, dict):
                              for key in ("name", "id", "backend", "alias"):
                                  value = item.get(key)
                                  if value:
                                      names.add(str(value))
                  return names

              def wait_for_localai_ready():
                  deadline = time.time() + 300
                  while time.time() < deadline:
                      try:
                          get_installed_models()
                          return
                      except Exception as exc:
                          print(f"LocalAI not ready yet: {exc}", flush=True)
                          time.sleep(5)
                  raise RuntimeError("Timed out waiting for LocalAI readiness")

              def ensure_backend_installed():
                  installed_backends = get_installed_backends()
                  if backend_name in installed_backends:
                      print(f"Backend {backend_name} already installed", flush=True)
                      return

                  print(f"Installing backend {backend_name}", flush=True)
                  http_request("POST", "/backends/apply", {"name": backend_name})

                  deadline = time.time() + timeout_seconds
                  while time.time() < deadline:
                      installed_backends = get_installed_backends()
                      if backend_name in installed_backends:
                          print(f"Backend {backend_name} installed successfully", flush=True)
                          return
                      print(f"Waiting for backend {backend_name} to become available...", flush=True)
                      time.sleep(poll_seconds)

                  raise RuntimeError(f"Backend {backend_name} install did not complete before timeout")

              def parse_job_status(job_response, job_id):
                  if isinstance(job_response, dict) and "processed" in job_response:
                      return job_response
                  if isinstance(job_response, dict) and job_id in job_response:
                      return job_response[job_id]
                  if isinstance(job_response, dict):
                      for value in job_response.values():
                          if isinstance(value, dict) and "processed" in value:
                              return value
                  raise RuntimeError(f"Unexpected job payload: {job_response}")

              def read_entries_from_file(file_path, label):
                  if not file_path:
                      return []
                  if not os.path.exists(file_path):
                      print(
                          f"{label} file not found: {file_path} (skipping)",
                          flush=True,
                      )
                      return []

                  entries = []
                  with open(file_path, "r", encoding="utf-8") as handle:
                      for raw_line in handle:
                          line = raw_line.strip()
                          if not line or line.startswith("#"):
                              continue
                          entries.append(line)
                  return entries

              def read_extra_model_urls():
                  return read_entries_from_file(model_urls_file, "Extra model URL")

              def read_extra_model_repos():
                  return read_entries_from_file(model_repos_file, "Extra model repo")

              def file_name_from_url(url):
                  parsed = urllib.parse.urlparse(url)
                  file_name = os.path.basename(parsed.path)
                  file_name = urllib.parse.unquote(file_name)
                  if not file_name:
                      raise RuntimeError(f"Unable to determine file name from URL: {url}")
                  return file_name

              def download_file(url, target_path, attempts):
                  for attempt in range(1, attempts + 1):
                      print(
                          f"Downloading {url} -> {target_path} (attempt {attempt}/{attempts})",
                          flush=True,
                      )
                      result = subprocess.run(
                          ["wget", "-c", "-O", target_path, url],
                          capture_output=True,
                          text=True,
                      )
                      if result.returncode == 0:
                          print(f"Download complete: {target_path}", flush=True)
                          return
                      if (
                          "416 Requested Range Not Satisfiable" in (result.stderr or "")
                          and os.path.exists(target_path)
                          and os.path.getsize(target_path) > 0
                      ):
                          print(
                              f"Download already complete (HTTP 416 with existing file): {target_path}",
                              flush=True,
                          )
                          return
                      print(
                          f"download failed rc={result.returncode}: {result.stderr.strip()}",
                          flush=True,
                      )
                      if attempt < attempts:
                          time.sleep(10)
                  raise RuntimeError(f"Failed to download file: {target_path}")

              def parse_repo_spec(spec):
                  repo_spec = spec
                  target_dir = ""

                  if "->" in spec:
                      repo_spec, target_dir = spec.split("->", 1)
                      repo_spec = repo_spec.strip()
                      target_dir = target_dir.strip()

                  revision = None
                  if "@" in repo_spec:
                      repo_id, revision = repo_spec.rsplit("@", 1)
                      repo_id = repo_id.strip()
                      revision = revision.strip() or None
                  else:
                      repo_id = repo_spec.strip()

                  if not repo_id:
                      raise RuntimeError(f"Invalid repo spec: {spec}")

                  if not target_dir:
                      target_dir = repo_id.rsplit("/", 1)[-1]
                  target_dir = os.path.basename(target_dir.strip("/"))
                  if not target_dir:
                      raise RuntimeError(f"Invalid target directory in repo spec: {spec}")

                  return repo_id, revision, target_dir

              def get_snapshot_download():
                  try:
                      from huggingface_hub import snapshot_download
                      return snapshot_download
                  except Exception:
                      print("Installing huggingface_hub...", flush=True)
                      install = subprocess.run(
                          [sys.executable, "-m", "pip", "install", "--no-cache-dir", "huggingface_hub"],
                          capture_output=True,
                          text=True,
                      )
                      if install.returncode != 0:
                          raise RuntimeError(
                              f"Failed to install huggingface_hub: {install.stderr.strip()}"
                          )
                      from huggingface_hub import snapshot_download
                      return snapshot_download

              def download_repo_snapshot(snapshot_download, repo_id, revision, target_path, attempts):
                  for attempt in range(1, attempts + 1):
                      try:
                          kwargs = {
                              "repo_id": repo_id,
                              "local_dir": target_path,
                              "resume_download": True,
                          }
                          if revision:
                              kwargs["revision"] = revision
                          if hf_token:
                              kwargs["token"] = hf_token
                          print(
                              f"Snapshot {repo_id}{'@' + revision if revision else ''} -> {target_path} (attempt {attempt}/{attempts})",
                              flush=True,
                          )
                          snapshot_download(**kwargs)
                          print(f"Snapshot complete: {target_path}", flush=True)
                          return
                      except Exception as exc:
                          print(f"snapshot failed: {exc}", flush=True)
                          if attempt < attempts:
                              time.sleep(10)
                  raise RuntimeError(f"Failed to snapshot repo {repo_id}")

              def prefetch_model_file_if_configured():
                  if not prefetch_file_url or not prefetch_file_name:
                      return
                  target_path = os.path.join("/models", prefetch_file_name)
                  download_file(prefetch_file_url, target_path, prefetch_attempts)

              def download_extra_model_files_if_configured():
                  extra_urls = read_extra_model_urls()
                  if not extra_urls:
                      print("No extra model URLs configured", flush=True)
                      return

                  print(
                      f"Downloading {len(extra_urls)} extra model file(s) from {model_urls_file}",
                      flush=True,
                  )
                  for url in extra_urls:
                      file_name = file_name_from_url(url)
                      target_path = os.path.join("/models", file_name)
                      download_file(url, target_path, prefetch_attempts)

              def download_extra_model_repos_if_configured():
                  repo_specs = read_extra_model_repos()
                  if not repo_specs:
                      print("No extra model repos configured", flush=True)
                      return

                  snapshot_download = get_snapshot_download()
                  print(
                      f"Downloading {len(repo_specs)} Hugging Face repo snapshot(s) from {model_repos_file}",
                      flush=True,
                  )
                  for spec in repo_specs:
                      repo_id, revision, target_dir = parse_repo_spec(spec)
                      target_path = os.path.join("/models", target_dir)
                      download_repo_snapshot(
                          snapshot_download,
                          repo_id,
                          revision,
                          target_path,
                          prefetch_attempts,
                      )

              wait_for_localai_ready()
              ensure_backend_installed()
              download_extra_model_files_if_configured()
              download_extra_model_repos_if_configured()
              installed = get_installed_models()
              if model_name in installed:
                  print(f"Model {model_name} already installed", flush=True)
                  sys.exit(0)

              prefetch_model_file_if_configured()

              last_error = None
              for attempt in range(1, max_install_attempts + 1):
                  # Clean partial files left by failed downloads before retrying.
                  try:
                      for file_name in os.listdir("/models"):
                          if file_name.endswith(".partial"):
                              os.remove(os.path.join("/models", file_name))
                  except Exception as exc:
                      print(f"warning: failed to clean partial files: {exc}", flush=True)

                  print(
                      f"Installing model {model_name} from {model_gallery_url} (attempt {attempt}/{max_install_attempts})",
                      flush=True,
                  )
                  apply_response = http_request(
                      "POST",
                      "/models/apply",
                      {"id": model_name, "url": model_gallery_url},
                  )
                  job_id = apply_response.get("uuid")
                  if not job_id:
                      raise RuntimeError(f"Missing job UUID in response: {apply_response}")

                  deadline = time.time() + timeout_seconds
                  while time.time() < deadline:
                      job_response = http_request("GET", f"/models/jobs/{job_id}")
                      status = parse_job_status(job_response, job_id)
                      processed = bool(status.get("processed"))
                      error = status.get("error")
                      message = status.get("message", "")
                      progress = status.get("progress", 0)
                      print(
                          f"job={job_id} processed={processed} progress={progress} message={message}",
                          flush=True,
                      )
                      if processed:
                          if error in (None, {}, ""):
                              installed = get_installed_models()
                              if model_name in installed:
                                  print(f"Model {model_name} installed successfully", flush=True)
                                  sys.exit(0)
                              last_error = (
                                  f"install job completed but model {model_name} is still missing"
                              )
                          else:
                              last_error = f"{error} {message}"
                          break
                      time.sleep(poll_seconds)
                  else:
                      last_error = f"timed out waiting for model install job {job_id}"

                  if last_error and "unexpected EOF" in str(last_error) and attempt < max_install_attempts:
                      print(f"Retrying after transient error: {last_error}", flush=True)
                      time.sleep(10)
                      continue
                  if attempt < max_install_attempts:
                      print(f"Retrying after failure: {last_error}", flush=True)
                      time.sleep(10)
                      continue

              raise RuntimeError(f"Failed to install model {model_name}: {last_error}")
              PY
      volumes:
        - name: models
          persistentVolumeClaim:
            claimName: localai-models
        - name: model-urls
          secret:
            secretName: localai-model-downloads
            optional: true
