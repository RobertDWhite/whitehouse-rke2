apiVersion: batch/v1
kind: Job
metadata:
  name: localai-image-model-bootstrap
  namespace: ai-stack
  annotations:
    argocd.argoproj.io/hook: PostSync
    argocd.argoproj.io/hook-delete-policy: BeforeHookCreation,HookSucceeded
spec:
  backoffLimit: 2
  ttlSecondsAfterFinished: 300
  template:
    spec:
      nodeSelector:
        gpu: "true"
      tolerations:
        - key: "node-role.kubernetes.io/control-plane"
          operator: "Exists"
          effect: "NoSchedule"
      restartPolicy: OnFailure
      containers:
        - name: bootstrap
          image: python:3.12-alpine
          env:
            - name: LOCALAI_BASE_URL
              value: "http://localai.ai-stack.svc.cluster.local:8080"
            - name: LOCALAI_IMAGE_MODEL_NAME
              value: "dreamshaper"
            - name: LOCALAI_IMAGE_BACKEND_NAME
              value: "diffusers"
            - name: LOCALAI_IMAGE_MODEL_GALLERY_URL
              value: "github:mudler/LocalAI/gallery/dreamshaper.yaml@master"
            - name: LOCALAI_BOOTSTRAP_TIMEOUT_SECONDS
              value: "7200"
            - name: LOCALAI_BOOTSTRAP_POLL_SECONDS
              value: "15"
            - name: LOCALAI_MAX_INSTALL_ATTEMPTS
              value: "4"
            - name: LOCALAI_HTTP_MAX_ATTEMPTS
              value: "6"
            - name: LOCALAI_HTTP_TIMEOUT_SECONDS
              value: "90"
            - name: LOCALAI_PREFETCH_ATTEMPTS
              value: "8"
            - name: LOCALAI_IMAGE_MODEL_FILE_URL
              value: "https://huggingface.co/Lykon/DreamShaper/resolve/main/DreamShaper_8_pruned.safetensors"
            - name: LOCALAI_IMAGE_MODEL_FILE_NAME
              value: "DreamShaper_8_pruned.safetensors"
          volumeMounts:
            - name: models
              mountPath: /models
          command:
            - /bin/sh
            - -lc
          args:
            - |
              python - <<'PY'
              import json
              import os
              import subprocess
              import sys
              import time
              import urllib.error
              import urllib.request

              base_url = os.getenv("LOCALAI_BASE_URL", "http://localai:8080").rstrip("/")
              model_name = os.getenv("LOCALAI_IMAGE_MODEL_NAME", "dreamshaper")
              backend_name = os.getenv("LOCALAI_IMAGE_BACKEND_NAME", "diffusers")
              model_gallery_url = os.getenv(
                  "LOCALAI_IMAGE_MODEL_GALLERY_URL",
                  "github:mudler/LocalAI/gallery/dreamshaper.yaml@master",
              )
              timeout_seconds = int(os.getenv("LOCALAI_BOOTSTRAP_TIMEOUT_SECONDS", "7200"))
              poll_seconds = int(os.getenv("LOCALAI_BOOTSTRAP_POLL_SECONDS", "15"))
              max_install_attempts = int(os.getenv("LOCALAI_MAX_INSTALL_ATTEMPTS", "4"))
              max_http_attempts = int(os.getenv("LOCALAI_HTTP_MAX_ATTEMPTS", "6"))
              http_timeout_seconds = int(os.getenv("LOCALAI_HTTP_TIMEOUT_SECONDS", "90"))
              prefetch_attempts = int(os.getenv("LOCALAI_PREFETCH_ATTEMPTS", "8"))
              prefetch_file_url = os.getenv("LOCALAI_IMAGE_MODEL_FILE_URL", "").strip()
              prefetch_file_name = os.getenv("LOCALAI_IMAGE_MODEL_FILE_NAME", "").strip()

              def http_request(method, path, payload=None):
                  data = None
                  headers = {}
                  if payload is not None:
                      data = json.dumps(payload).encode("utf-8")
                      headers["Content-Type"] = "application/json"
                  req = urllib.request.Request(
                      f"{base_url}{path}",
                      data=data,
                      method=method,
                      headers=headers,
                  )
                  last_exc = None
                  for attempt in range(1, max_http_attempts + 1):
                      try:
                          with urllib.request.urlopen(req, timeout=http_timeout_seconds) as resp:
                              body = resp.read().decode("utf-8")
                          if not body:
                              return {}
                          return json.loads(body)
                      except Exception as exc:
                          last_exc = exc
                          if attempt < max_http_attempts:
                              print(
                                  f"http retry {attempt}/{max_http_attempts} for {method} {path}: {exc}",
                                  flush=True,
                              )
                              time.sleep(5)
                              continue
                  raise last_exc

              def get_installed_models():
                  response = http_request("GET", "/v1/models")
                  data = response.get("data", [])
                  names = set()
                  for item in data:
                      if isinstance(item, dict):
                          if item.get("id"):
                              names.add(str(item["id"]))
                          if item.get("name"):
                              names.add(str(item["name"]))
                  return names

              def get_installed_backends():
                  response = http_request("GET", "/backends")
                  names = set()
                  if isinstance(response, list):
                      for item in response:
                          if isinstance(item, str):
                              names.add(item)
                          elif isinstance(item, dict):
                              for key in ("name", "id", "backend", "alias"):
                                  value = item.get(key)
                                  if value:
                                      names.add(str(value))
                  return names

              def wait_for_localai_ready():
                  deadline = time.time() + 300
                  while time.time() < deadline:
                      try:
                          get_installed_models()
                          return
                      except Exception as exc:
                          print(f"LocalAI not ready yet: {exc}", flush=True)
                          time.sleep(5)
                  raise RuntimeError("Timed out waiting for LocalAI readiness")

              def ensure_backend_installed():
                  installed_backends = get_installed_backends()
                  if backend_name in installed_backends:
                      print(f"Backend {backend_name} already installed", flush=True)
                      return

                  print(f"Installing backend {backend_name}", flush=True)
                  http_request("POST", "/backends/apply", {"name": backend_name})

                  deadline = time.time() + timeout_seconds
                  while time.time() < deadline:
                      installed_backends = get_installed_backends()
                      if backend_name in installed_backends:
                          print(f"Backend {backend_name} installed successfully", flush=True)
                          return
                      print(f"Waiting for backend {backend_name} to become available...", flush=True)
                      time.sleep(poll_seconds)

                  raise RuntimeError(f"Backend {backend_name} install did not complete before timeout")

              def parse_job_status(job_response, job_id):
                  if isinstance(job_response, dict) and "processed" in job_response:
                      return job_response
                  if isinstance(job_response, dict) and job_id in job_response:
                      return job_response[job_id]
                  if isinstance(job_response, dict):
                      for value in job_response.values():
                          if isinstance(value, dict) and "processed" in value:
                              return value
                  raise RuntimeError(f"Unexpected job payload: {job_response}")

              def prefetch_model_file_if_configured():
                  if not prefetch_file_url or not prefetch_file_name:
                      return
                  target_path = os.path.join("/models", prefetch_file_name)
                  for attempt in range(1, prefetch_attempts + 1):
                      print(
                          f"Prefetching model file {target_path} (attempt {attempt}/{prefetch_attempts})",
                          flush=True,
                      )
                      result = subprocess.run(
                          ["wget", "-c", "-O", target_path, prefetch_file_url],
                          capture_output=True,
                          text=True,
                      )
                      if result.returncode == 0:
                          print(f"Prefetch complete: {target_path}", flush=True)
                          return
                      if (
                          "416 Requested Range Not Satisfiable" in (result.stderr or "")
                          and os.path.exists(target_path)
                          and os.path.getsize(target_path) > 0
                      ):
                          print(
                              f"Prefetch already complete (HTTP 416 with existing file): {target_path}",
                              flush=True,
                          )
                          return
                      print(
                          f"prefetch failed rc={result.returncode}: {result.stderr.strip()}",
                          flush=True,
                      )
                      if attempt < prefetch_attempts:
                          time.sleep(10)
                  raise RuntimeError(f"Failed to prefetch model file: {target_path}")

              wait_for_localai_ready()
              ensure_backend_installed()
              installed = get_installed_models()
              if model_name in installed:
                  print(f"Model {model_name} already installed", flush=True)
                  sys.exit(0)

              prefetch_model_file_if_configured()

              last_error = None
              for attempt in range(1, max_install_attempts + 1):
                  # Clean partial files left by failed downloads before retrying.
                  try:
                      for file_name in os.listdir("/models"):
                          if file_name.endswith(".partial"):
                              os.remove(os.path.join("/models", file_name))
                  except Exception as exc:
                      print(f"warning: failed to clean partial files: {exc}", flush=True)

                  print(
                      f"Installing model {model_name} from {model_gallery_url} (attempt {attempt}/{max_install_attempts})",
                      flush=True,
                  )
                  apply_response = http_request(
                      "POST",
                      "/models/apply",
                      {"id": model_name, "url": model_gallery_url},
                  )
                  job_id = apply_response.get("uuid")
                  if not job_id:
                      raise RuntimeError(f"Missing job UUID in response: {apply_response}")

                  deadline = time.time() + timeout_seconds
                  while time.time() < deadline:
                      job_response = http_request("GET", f"/models/jobs/{job_id}")
                      status = parse_job_status(job_response, job_id)
                      processed = bool(status.get("processed"))
                      error = status.get("error")
                      message = status.get("message", "")
                      progress = status.get("progress", 0)
                      print(
                          f"job={job_id} processed={processed} progress={progress} message={message}",
                          flush=True,
                      )
                      if processed:
                          if error in (None, {}, ""):
                              installed = get_installed_models()
                              if model_name in installed:
                                  print(f"Model {model_name} installed successfully", flush=True)
                                  sys.exit(0)
                              last_error = (
                                  f"install job completed but model {model_name} is still missing"
                              )
                          else:
                              last_error = f"{error} {message}"
                          break
                      time.sleep(poll_seconds)
                  else:
                      last_error = f"timed out waiting for model install job {job_id}"

                  if last_error and "unexpected EOF" in str(last_error) and attempt < max_install_attempts:
                      print(f"Retrying after transient error: {last_error}", flush=True)
                      time.sleep(10)
                      continue
                  if attempt < max_install_attempts:
                      print(f"Retrying after failure: {last_error}", flush=True)
                      time.sleep(10)
                      continue

              raise RuntimeError(f"Failed to install model {model_name}: {last_error}")
              PY
      volumes:
        - name: models
          persistentVolumeClaim:
            claimName: localai-models
