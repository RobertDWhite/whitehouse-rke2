apiVersion: v1
data:
  audio.py: |
    import numpy as np
    import librosa
    import librosa.display
    import matplotlib
    matplotlib.use("Agg")
    import matplotlib.pyplot as plt


    def generate_waveform_peaks(audio_path: str, num_peaks: int = 1000) -> dict:
        """Generate waveform peaks for visualization.

        Returns a dict with:
        - peaks: list of [min, max] values per segment
        - duration: total duration in seconds
        - sample_rate: original sample rate
        """
        # Load audio
        y, sr = librosa.load(audio_path, sr=None, mono=True)
        duration = len(y) / sr

        # Calculate samples per peak
        samples_per_peak = max(1, len(y) // num_peaks)

        peaks = []
        for i in range(0, len(y), samples_per_peak):
            segment = y[i : i + samples_per_peak]
            if len(segment) > 0:
                peaks.append([float(np.min(segment)), float(np.max(segment))])

        return {
            "peaks": peaks,
            "duration": duration,
            "sample_rate": sr,
            "num_samples": len(y),
        }


    def generate_spectrogram(
        audio_path: str,
        output_path: str,
        figsize: tuple = (12, 4),
        max_seconds: int = 90,
    ):
        """Generate spectrogram image and save to file."""
        # Bound decode duration to avoid huge memory/latency on very long captures.
        y, sr = librosa.load(audio_path, sr=8000, mono=True, duration=max_seconds)

        # Compute spectrogram
        hop_length = max(256, len(y) // 2000) if len(y) else 256
        D = librosa.amplitude_to_db(
            np.abs(librosa.stft(y, n_fft=512, hop_length=hop_length)),
            ref=np.max,
        )

        # Create figure
        fig, ax = plt.subplots(figsize=figsize)
        img = librosa.display.specshow(D, sr=sr, x_axis="time", y_axis="hz", ax=ax, cmap="magma")
        ax.set_xlabel("Time (s)")
        ax.set_ylabel("Frequency (Hz)")
        fig.colorbar(img, ax=ax, format="%+2.0f dB")

        # Save
        plt.tight_layout()
        plt.savefig(output_path, dpi=100, bbox_inches="tight", pad_inches=0.1)
        plt.close(fig)


    def get_time_segments(transcript: str, duration: float) -> list:
        """Estimate time segments for transcript words.

        Since Whisper doesn't always provide word-level timestamps in our setup,
        we estimate based on character position and total duration.
        """
        if not transcript:
            return []

        words = transcript.split()
        total_chars = len(transcript)
        if total_chars == 0:
            return []

        segments = []
        char_pos = 0
        for word in words:
            start_time = (char_pos / total_chars) * duration
            char_pos += len(word) + 1  # +1 for space
            end_time = (char_pos / total_chars) * duration
            segments.append({
                "word": word,
                "start": start_time,
                "end": end_time,
            })

        return segments
  files.py: |
    import os
    from datetime import datetime
    from typing import Optional, List

    from fastapi import APIRouter, Depends, HTTPException, Query, Request
    from fastapi.responses import StreamingResponse, FileResponse
    from pydantic import BaseModel
    from sqlalchemy import desc, func, or_
    from sqlalchemy.orm import Session

    from ..database import get_db
    from ..models import CallsignInfo, Recording, Repeater
    from ..services.tagging import extract_callsign_tags, normalize_callsign, parse_ai_tags

    router = APIRouter()


    class UpdateRecordingRequest(BaseModel):
        ai_tags: Optional[List[str]] = None


    class BulkDeleteRequest(BaseModel):
        ids: List[int]


    class BulkDeleteFilteredRequest(BaseModel):
        mode: Optional[str] = None
        frequency_min: Optional[float] = None
        frequency_max: Optional[float] = None
        q: Optional[str] = None
        callsign: Optional[str] = None
        date_from: Optional[datetime] = None
        date_to: Optional[datetime] = None
        duration_min: Optional[float] = None
        duration_max: Optional[float] = None
        has_transcript: Optional[bool] = None
        dry_run: bool = False


    def _validate_mode(mode: Optional[str]):
        if mode is not None and mode not in {"cw", "voice"}:
            raise HTTPException(status_code=422, detail="mode must be 'cw' or 'voice'")


    def _validate_duration_bounds(duration_min: Optional[float], duration_max: Optional[float]):
        if (
            duration_min is not None
            and duration_max is not None
            and duration_min > duration_max
        ):
            raise HTTPException(
                status_code=422,
                detail="duration_min cannot be greater than duration_max",
            )


    def _all_tags(callsign_tags: List[str], ai_tags: List[str]) -> List[str]:
        return list(dict.fromkeys(callsign_tags + ai_tags))


    def _apply_recording_filters(
        query,
        mode: Optional[str] = None,
        frequency_min: Optional[float] = None,
        frequency_max: Optional[float] = None,
        q: Optional[str] = None,
        callsign: Optional[str] = None,
        date_from: Optional[datetime] = None,
        date_to: Optional[datetime] = None,
        duration_min: Optional[float] = None,
        duration_max: Optional[float] = None,
        has_transcript: Optional[bool] = None,
    ):
        if mode:
            query = query.filter(Recording.mode == mode)
        if frequency_min is not None:
            query = query.filter(Recording.frequency_hz >= frequency_min)
        if frequency_max is not None:
            query = query.filter(Recording.frequency_hz <= frequency_max)

        if q:
            cleaned_q = q.strip()
            if cleaned_q:
                search_query = func.plainto_tsquery("english", cleaned_q)
                query = query.filter(
                    or_(
                        Recording.search_vector.op("@@")(search_query),
                        Recording.transcript.ilike(f"%{cleaned_q}%"),
                    )
                )

        if callsign:
            normalized_callsign = normalize_callsign(callsign)
            if normalized_callsign:
                normalized_transcript = func.regexp_replace(
                    func.upper(func.coalesce(Recording.transcript, "")),
                    "[^A-Z0-9]+",
                    "",
                    "g",
                )
                query = query.filter(normalized_transcript.ilike(f"%{normalized_callsign}%"))

        if date_from:
            query = query.filter(Recording.timestamp >= date_from)
        if date_to:
            query = query.filter(Recording.timestamp <= date_to)
        if duration_min is not None:
            query = query.filter(Recording.duration_seconds >= duration_min)
        if duration_max is not None:
            query = query.filter(Recording.duration_seconds <= duration_max)

        transcript_len = func.length(func.trim(func.coalesce(Recording.transcript, "")))
        if has_transcript is True:
            query = query.filter(transcript_len > 0)
        elif has_transcript is False:
            query = query.filter(transcript_len == 0)

        return query


    def _apply_tag_filter(query, tag: Optional[str]):
        """Filter by an AI or callsign tag substring present in the JSON tags list."""
        if not tag:
            return query
        cleaned = tag.strip().lower()
        if not cleaned:
            return query
        # ai_tags is a JSON-encoded list like ["repeater","W3RDW","voice"]
        # Match tag as a JSON string element, case-insensitive.
        query = query.filter(Recording.ai_tags.ilike(f'%"{cleaned}"%'))
        return query


    @router.get("/browse")
    async def browse_files(
        mode: Optional[str] = Query(None, pattern="^(cw|voice)$"),
        frequency_min: Optional[float] = None,
        frequency_max: Optional[float] = None,
        q: Optional[str] = Query(None, min_length=1),
        callsign: Optional[str] = Query(None, min_length=1, max_length=20),
        tag: Optional[str] = Query(None, min_length=1, max_length=50),
        date_from: Optional[datetime] = None,
        date_to: Optional[datetime] = None,
        duration_min: Optional[float] = Query(None, ge=0),
        duration_max: Optional[float] = Query(None, ge=0),
        has_transcript: Optional[bool] = None,
        page: int = Query(1, ge=1),
        limit: int = Query(50, ge=1, le=200),
        db: Session = Depends(get_db),
    ):
        """List recordings with optional filters."""
        _validate_duration_bounds(duration_min, duration_max)

        query = _apply_recording_filters(
            db.query(Recording),
            mode=mode,
            frequency_min=frequency_min,
            frequency_max=frequency_max,
            q=q,
            callsign=callsign,
            date_from=date_from,
            date_to=date_to,
            duration_min=duration_min,
            duration_max=duration_max,
            has_transcript=has_transcript,
        )
        query = _apply_tag_filter(query, tag)

        total = query.count()
        recordings = (
            query.order_by(desc(Recording.timestamp))
            .offset((page - 1) * limit)
            .limit(limit)
            .all()
        )

        return {
            "total": total,
            "page": page,
            "limit": limit,
            "items": [
                {
                    "id": r.id,
                    "filename": r.filename,
                    "mode": r.mode,
                    "frequency_hz": r.frequency_hz,
                    "frequency_label": r.frequency_label,
                    "timestamp": r.timestamp.isoformat() if r.timestamp else None,
                    "duration_seconds": r.duration_seconds,
                    "has_transcript": bool((r.transcript or "").strip()),
                    "callsign_tags": (callsign_tags := extract_callsign_tags(r.transcript)),
                    "ai_tags": (ai_tags := parse_ai_tags(r.ai_tags)),
                    "tags": _all_tags(callsign_tags, ai_tags),
                }
                for r in recordings
            ],
        }


    @router.get("/{file_id}")
    async def get_file(file_id: int, db: Session = Depends(get_db)):
        """Get file metadata and transcript."""
        recording = db.query(Recording).filter(Recording.id == file_id).first()
        if not recording:
            raise HTTPException(status_code=404, detail="Recording not found")

        callsign_tags = extract_callsign_tags(recording.transcript)
        ai_tags = parse_ai_tags(recording.ai_tags)

        # Repeater info
        repeater_info = None
        if recording.repeater_id:
            rep = db.query(Repeater).filter(Repeater.id == recording.repeater_id).first()
            if rep:
                repeater_info = {
                    "id": rep.id,
                    "callsign": rep.callsign,
                    "location": rep.location,
                    "county": rep.county,
                    "state": rep.state,
                    "frequency_hz": rep.frequency_hz,
                    "input_hz": rep.input_hz,
                    "pl_tone": rep.pl_tone,
                    "digital_modes": [m.strip() for m in rep.digital_modes.split(",")] if rep.digital_modes else [],
                    "linked_nodes": rep.linked_nodes,
                    "use": rep.use,
                }

        # Operator info for callsigns heard in transcript
        operators = []
        for cs in callsign_tags[:5]:
            info = db.query(CallsignInfo).filter(CallsignInfo.callsign == cs).first()
            if info:
                operators.append({
                    "callsign": info.callsign,
                    "name": info.name,
                    "qth_city": info.qth_city,
                    "qth_state": info.qth_state,
                    "license_class": info.license_class,
                    "grid": info.grid,
                })

        return {
            "id": recording.id,
            "filename": recording.filename,
            "mode": recording.mode,
            "frequency_hz": recording.frequency_hz,
            "timestamp": recording.timestamp.isoformat() if recording.timestamp else None,
            "duration_seconds": recording.duration_seconds,
            "transcript": recording.transcript,
            "frequency_label": recording.frequency_label,
            "callsign_tags": callsign_tags,
            "ai_tags": ai_tags,
            "tags": _all_tags(callsign_tags, ai_tags),
            "has_waveform": recording.waveform_cached is not None,
            "has_spectrogram": recording.spectrogram_cached is not None,
            "repeater": repeater_info,
            "operators": operators,
        }


    @router.patch("/{file_id}")
    async def update_recording(
        file_id: int, body: UpdateRecordingRequest, db: Session = Depends(get_db)
    ):
        """Update mutable fields on a recording (currently: ai_tags)."""
        recording = db.query(Recording).filter(Recording.id == file_id).first()
        if not recording:
            raise HTTPException(status_code=404, detail="Recording not found")

        if body.ai_tags is not None:
            from ..services.tagging import dump_ai_tags
            recording.ai_tags = dump_ai_tags(body.ai_tags)

        db.commit()
        ai_tags = parse_ai_tags(recording.ai_tags)
        callsign_tags = extract_callsign_tags(recording.transcript)
        return {
            "id": recording.id,
            "ai_tags": ai_tags,
            "callsign_tags": callsign_tags,
            "tags": _all_tags(callsign_tags, ai_tags),
        }


    @router.get("/{file_id}/stream")
    async def stream_file(file_id: int, request: Request, db: Session = Depends(get_db)):
        """Stream audio file with Range request support."""
        recording = db.query(Recording).filter(Recording.id == file_id).first()
        if not recording:
            raise HTTPException(status_code=404, detail="Recording not found")

        audio_path = recording.audio_path
        if not os.path.exists(audio_path):
            raise HTTPException(status_code=404, detail="Audio file not found")

        file_size = os.path.getsize(audio_path)

        # Handle Range requests for seeking
        range_header = request.headers.get("Range")
        if range_header:
            try:
                range_spec = range_header.replace("bytes=", "")
                start, end = range_spec.split("-")
                start = int(start) if start else 0
                end = int(end) if end else file_size - 1
                end = min(end, file_size - 1)

                content_length = end - start + 1

                def iter_file():
                    with open(audio_path, "rb") as f:
                        f.seek(start)
                        remaining = content_length
                        while remaining > 0:
                            chunk_size = min(8192, remaining)
                            data = f.read(chunk_size)
                            if not data:
                                break
                            remaining -= len(data)
                            yield data

                return StreamingResponse(
                    iter_file(),
                    status_code=206,
                    media_type="audio/wav",
                    headers={
                        "Content-Range": f"bytes {start}-{end}/{file_size}",
                        "Content-Length": str(content_length),
                        "Accept-Ranges": "bytes",
                    },
                )
            except (ValueError, IndexError):
                pass

        # Full file response
        return FileResponse(
            audio_path,
            media_type="audio/wav",
            headers={"Accept-Ranges": "bytes"},
        )


    def _delete_recording_files(recording: Recording):
        """Remove audio, text, and cache files for a recording."""
        for path in [
            recording.audio_path,
            recording.text_path,
            recording.waveform_cached,
            recording.spectrogram_cached,
        ]:
            if path:
                try:
                    os.remove(path)
                except OSError:
                    pass


    @router.delete("/{file_id}")
    async def delete_file(file_id: int, db: Session = Depends(get_db)):
        """Delete a single recording and its files."""
        recording = db.query(Recording).filter(Recording.id == file_id).first()
        if not recording:
            raise HTTPException(status_code=404, detail="Recording not found")

        _delete_recording_files(recording)
        db.delete(recording)
        db.commit()
        return {"deleted": 1}


    @router.post("/bulk-delete")
    async def bulk_delete_files(body: BulkDeleteRequest, db: Session = Depends(get_db)):
        """Delete multiple recordings by ID."""
        recordings = db.query(Recording).filter(Recording.id.in_(body.ids)).all()
        count = 0
        for recording in recordings:
            _delete_recording_files(recording)
            db.delete(recording)
            count += 1
        db.commit()
        return {"deleted": count}


    @router.post("/bulk-delete-filtered")
    async def bulk_delete_filtered_files(
        body: BulkDeleteFilteredRequest,
        db: Session = Depends(get_db),
    ):
        """Delete all recordings matching the provided filter criteria."""
        _validate_mode(body.mode)
        _validate_duration_bounds(body.duration_min, body.duration_max)

        has_any_filter = any(
            [
                body.mode is not None,
                body.frequency_min is not None,
                body.frequency_max is not None,
                body.q is not None,
                body.callsign is not None,
                body.date_from is not None,
                body.date_to is not None,
                body.duration_min is not None,
                body.duration_max is not None,
                body.has_transcript is not None,
            ]
        )
        if not has_any_filter:
            raise HTTPException(
                status_code=400,
                detail="At least one filter is required for bulk-delete-filtered",
            )

        query = _apply_recording_filters(
            db.query(Recording),
            mode=body.mode,
            frequency_min=body.frequency_min,
            frequency_max=body.frequency_max,
            q=body.q,
            callsign=body.callsign,
            date_from=body.date_from,
            date_to=body.date_to,
            duration_min=body.duration_min,
            duration_max=body.duration_max,
            has_transcript=body.has_transcript,
        )
        recordings = query.all()
        matched = len(recordings)

        if body.dry_run:
            return {"matched": matched, "deleted": 0}

        for recording in recordings:
            _delete_recording_files(recording)
            db.delete(recording)
        db.commit()

        return {"matched": matched, "deleted": matched}
  search.py: |
    from typing import Optional, List

    from fastapi import APIRouter, Depends, Query
    from sqlalchemy import func, text
    from sqlalchemy.orm import Session

    from ..database import get_db
    from ..models import Recording
    from ..services.tagging import extract_callsign_tags, normalize_callsign, parse_ai_tags

    router = APIRouter()


    @router.get("/text")
    async def search_text(
        q: str = Query(..., min_length=1),
        mode: Optional[str] = Query(None, pattern="^(cw|voice)$"),
        frequency_min: Optional[float] = None,
        frequency_max: Optional[float] = None,
        callsign: Optional[str] = Query(None, min_length=1, max_length=20),
        page: int = Query(1, ge=1),
        limit: int = Query(50, ge=1, le=200),
        db: Session = Depends(get_db),
    ):
        """Full-text search on transcripts."""
        search_query = func.plainto_tsquery("english", q)

        query = db.query(
            Recording,
            func.ts_rank(Recording.search_vector, search_query).label("rank"),
            func.ts_headline(
                "english",
                Recording.transcript,
                search_query,
                "StartSel=<mark>, StopSel=</mark>, MaxWords=50, MinWords=20",
            ).label("headline"),
        ).filter(Recording.search_vector.op("@@")(search_query))

        if mode:
            query = query.filter(Recording.mode == mode)
        if frequency_min is not None:
            query = query.filter(Recording.frequency_hz >= frequency_min)
        if frequency_max is not None:
            query = query.filter(Recording.frequency_hz <= frequency_max)

        if callsign:
            normalized_callsign = normalize_callsign(callsign)
            if normalized_callsign:
                normalized_transcript = func.regexp_replace(
                    func.upper(func.coalesce(Recording.transcript, "")),
                    "[^A-Z0-9]+",
                    "",
                    "g",
                )
                query = query.filter(normalized_transcript.ilike(f"%{normalized_callsign}%"))

        total = query.count()
        results = query.order_by(text("rank DESC")).offset((page - 1) * limit).limit(limit).all()

        return {
            "query": q,
            "total": total,
            "page": page,
            "limit": limit,
            "items": [
                {
                    "id": r.Recording.id,
                    "filename": r.Recording.filename,
                    "mode": r.Recording.mode,
                    "frequency_hz": r.Recording.frequency_hz,
                    "frequency_label": r.Recording.frequency_label,
                    "timestamp": r.Recording.timestamp.isoformat() if r.Recording.timestamp else None,
                    "headline": r.headline,
                    "rank": float(r.rank),
                    "callsign_tags": (callsign_tags := extract_callsign_tags(r.Recording.transcript)),
                    "ai_tags": (ai_tags := parse_ai_tags(r.Recording.ai_tags)),
                    "tags": list(dict.fromkeys(callsign_tags + ai_tags)),
                }
                for r in results
            ],
        }
  tagging.py: |
    import json
    import os
    import re
    from typing import List, Optional


    CALLSIGN_PATTERN = re.compile(r"\b(?:[AKNW][A-Z]?\d[A-Z]{1,3})\b", re.IGNORECASE)
    NO_SPEECH_PATTERN = re.compile(
        r"^\s*(?:"
        r"\[?\s*no speech detected\s*\]?"
        r"|\[?\s*recording too long for auto-transcribe\s*\]?"
        r"|[^\w\s]+"
        r"|thanks?\s+(?:for\s+)?(?:watching|listening)\.?"
        r"|(?:thank\s+you|bye|goodbye|see\s+you)\.?"
        r"|\[(?:music|applause|laughter|silence|noise|inaudible)[^\]]*\]"
        r")\s*$",
        re.IGNORECASE,
    )

    PHONETIC_MAP = {
        "ALFA": "A", "ALPHA": "A", "BRAVO": "B", "CHARLIE": "C", "DELTA": "D",
        "ECHO": "E", "FOXTROT": "F", "GOLF": "G", "HOTEL": "H", "INDIA": "I",
        "JULIETT": "J", "JULIET": "J", "KILO": "K", "LIMA": "L", "MIKE": "M",
        "NOVEMBER": "N", "OSCAR": "O", "PAPA": "P", "QUEBEC": "Q", "ROMEO": "R",
        "SIERRA": "S", "TANGO": "T", "UNIFORM": "U", "VICTOR": "V", "WHISKEY": "W",
        "XRAY": "X", "X-RAY": "X", "YANKEE": "Y", "ZULU": "Z",
        "ZERO": "0", "OH": "0", "ONE": "1", "TWO": "2", "TOO": "2",
        "THREE": "3", "TREE": "3", "FOUR": "4", "FIVE": "5", "SIX": "6",
        "SEVEN": "7", "EIGHT": "8", "NINE": "9", "NINER": "9",
    }


    def normalize_callsign(value: str) -> str:
        return re.sub(r"[^A-Z0-9]", "", value.upper())


    def callsign_hints() -> List[str]:
        raw = os.getenv("CALLSIGN_HINTS", "w3rdw")
        hints: List[str] = []
        for token in raw.split(","):
            normalized = normalize_callsign(token.strip())
            if normalized:
                hints.append(normalized)
        return hints


    def phonetic_compact(text: str) -> str:
        tokens = re.findall(r"[A-Z0-9-]+", text.upper())
        compact: List[str] = []
        for token in tokens:
            mapped = PHONETIC_MAP.get(token)
            if mapped:
                compact.append(mapped)
                continue
            if len(token) == 1 and token.isalnum():
                compact.append(token)
                continue
            if token.isdigit():
                compact.append(token)
        return "".join(compact)


    def extract_callsign_tags(transcript: Optional[str]) -> List[str]:
        if not transcript:
            return []
        tags: List[str] = []
        seen = set()
        for match in CALLSIGN_PATTERN.finditer(transcript):
            tag = normalize_callsign(match.group(0))
            if tag and tag not in seen:
                seen.add(tag)
                tags.append(tag)
        normalized_text = normalize_callsign(transcript)
        phonetic_text = phonetic_compact(transcript)
        for hint in callsign_hints():
            if (hint in normalized_text or hint in phonetic_text) and hint not in seen:
                seen.add(hint)
                tags.append(hint)
        return tags


    def is_no_speech_transcript(transcript: Optional[str]) -> bool:
        if not transcript:
            return False
        return NO_SPEECH_PATTERN.match(transcript) is not None


    def parse_ai_tags(raw: Optional[str]) -> List[str]:
        if not raw:
            return []
        try:
            value = json.loads(raw)
        except json.JSONDecodeError:
            return []
        if not isinstance(value, list):
            return []
        tags: List[str] = []
        seen = set()
        for item in value:
            if not isinstance(item, str):
                continue
            cleaned = item.strip()
            if cleaned and cleaned not in seen:
                seen.add(cleaned)
                tags.append(cleaned)
        return tags


    def dump_ai_tags(tags: List[str]) -> Optional[str]:
        cleaned: List[str] = []
        seen = set()
        for tag in tags:
            normalized = tag.strip()
            if normalized and normalized not in seen:
                seen.add(normalized)
                cleaned.append(normalized)
        if not cleaned:
            return None
        return json.dumps(cleaned)
  known_freqs.py: |
    """
    Static lookup table of well-known frequencies.

    Frequencies are stored in Hz with a match tolerance. The first match wins,
    so list more specific entries (narrow-band) before broader band ranges.
    Edit this configmap to add or adjust entries without rebuilding the image.
    """

    from typing import Optional


    # (center_hz, tolerance_hz, label)
    _KNOWN: list[tuple[float, float, str]] = [
        # ── NOAA Weather Radio ────────────────────────────────────────────────
        (162_400_000, 3_000, "NOAA WX-1"),
        (162_425_000, 3_000, "NOAA WX-2"),
        (162_450_000, 3_000, "NOAA WX-3"),
        (162_475_000, 3_000, "NOAA WX-4"),
        (162_500_000, 3_000, "NOAA WX-5"),
        (162_525_000, 3_000, "NOAA WX-6"),
        (162_550_000, 3_000, "NOAA WX-7"),

        # ── Amateur — 2m ──────────────────────────────────────────────────────
        (144_200_000, 3_000, "2m SSB Calling"),
        (144_390_000, 3_000, "APRS 2m"),
        (146_520_000, 5_000, "2m National Simplex Calling"),
        (146_580_000, 5_000, "2m FM Simplex"),
        (147_555_000, 5_000, "2m FM Simplex"),

        # ── Amateur — 70cm ────────────────────────────────────────────────────
        (432_100_000, 3_000, "70cm SSB Calling"),
        (433_920_000, 5_000, "70cm FM Simplex"),
        (446_000_000, 5_000, "70cm National Simplex Calling"),
        (446_500_000, 5_000, "70cm FM Simplex"),

        # ── Amateur — 1.25m ───────────────────────────────────────────────────
        (223_500_000, 5_000, "1.25m National Simplex Calling"),

        # ── ISS ───────────────────────────────────────────────────────────────
        (145_800_000, 5_000, "ISS Voice Downlink"),
        (437_550_000, 5_000, "ISS Packet"),
        (145_825_000, 3_000, "ISS APRS"),

        # ── Aviation ──────────────────────────────────────────────────────────
        (121_500_000, 5_000, "Aviation Distress (Guard)"),
        (122_750_000, 5_000, "Aviation Multicom"),
        (123_025_000, 5_000, "Aviation Unicom"),
        (123_450_000, 5_000, "Aviation Air-to-Air"),

        # ── Marine VHF ───────────────────────────────────────────────────────
        (156_800_000, 5_000, "Marine Ch 16 (Distress/Calling)"),
        (156_300_000, 5_000, "Marine Ch 6 (Safety)"),
        (157_050_000, 5_000, "Marine Ch 22A (USCG Working)"),

        # ── FRS/GMRS simplex ─────────────────────────────────────────────────
        (462_562_500, 5_000, "FRS/GMRS Ch 1"),
        (462_587_500, 5_000, "FRS/GMRS Ch 2"),
        (462_612_500, 5_000, "FRS/GMRS Ch 3"),
        (462_637_500, 5_000, "FRS/GMRS Ch 4"),
        (462_662_500, 5_000, "FRS/GMRS Ch 5"),
        (462_687_500, 5_000, "FRS/GMRS Ch 6"),
        (462_712_500, 5_000, "FRS/GMRS Ch 7"),
        (467_562_500, 5_000, "FRS Ch 8"),
        (467_587_500, 5_000, "FRS Ch 9"),
        (467_612_500, 5_000, "FRS Ch 10"),
        (467_637_500, 5_000, "FRS Ch 11"),
        (467_662_500, 5_000, "FRS Ch 12"),
        (467_687_500, 5_000, "FRS Ch 13"),
        (467_712_500, 5_000, "FRS Ch 14"),
        (462_550_000, 5_000, "GMRS Ch 15 (Calling)"),
        (462_575_000, 5_000, "GMRS Ch 16"),
        (462_600_000, 5_000, "GMRS Ch 17"),
        (462_625_000, 5_000, "GMRS Ch 18"),
        (462_650_000, 5_000, "GMRS Ch 19"),
        (462_675_000, 5_000, "GMRS Ch 20"),
        (462_700_000, 5_000, "GMRS Ch 21"),
        (462_725_000, 5_000, "GMRS Ch 22"),

        # ── MURS ─────────────────────────────────────────────────────────────
        (151_820_000, 5_000, "MURS Ch 1"),
        (151_880_000, 5_000, "MURS Ch 2"),
        (151_940_000, 5_000, "MURS Ch 3"),
        (154_570_000, 5_000, "MURS Ch 4"),
        (154_600_000, 5_000, "MURS Ch 5"),
    ]


    def lookup_known_freq(frequency_hz: float) -> Optional[str]:
        """Return a human label for a known frequency, or None if unrecognized."""
        for center, tolerance, label in _KNOWN:
            if abs(frequency_hz - center) <= tolerance:
                return label
        return None
  repeaters.py: |
    from typing import Optional

    from fastapi import APIRouter, Depends, Query
    from sqlalchemy import asc
    from sqlalchemy.orm import Session

    from ..database import get_db
    from ..models import Repeater

    router = APIRouter()


    @router.get("")
    async def list_repeaters(
        state: Optional[str] = Query(None, min_length=2, max_length=4),
        callsign: Optional[str] = Query(None, min_length=1, max_length=20),
        frequency_min: Optional[float] = None,
        frequency_max: Optional[float] = None,
        digital_only: bool = False,
        page: int = Query(1, ge=1),
        limit: int = Query(100, ge=1, le=2000),
        db: Session = Depends(get_db),
    ):
        """Browse known repeaters from the local RepeaterBook cache."""
        query = db.query(Repeater)

        if state:
            query = query.filter(Repeater.state.ilike(state))
        if callsign:
            query = query.filter(Repeater.callsign.ilike(f"%{callsign.upper()}%"))
        if frequency_min is not None:
            query = query.filter(Repeater.frequency_hz >= frequency_min)
        if frequency_max is not None:
            query = query.filter(Repeater.frequency_hz <= frequency_max)
        if digital_only:
            query = query.filter(
                Repeater.digital_modes.isnot(None),
                Repeater.digital_modes != "",
            )

        total = query.count()
        repeaters = (
            query.order_by(asc(Repeater.frequency_hz))
            .offset((page - 1) * limit)
            .limit(limit)
            .all()
        )

        return {
            "total": total,
            "page": page,
            "limit": limit,
            "items": [
                {
                    "id": r.id,
                    "callsign": r.callsign,
                    "frequency_hz": r.frequency_hz,
                    "input_hz": r.input_hz,
                    "pl_tone": r.pl_tone,
                    "location": r.location,
                    "county": r.county,
                    "state": r.state,
                    "latitude": r.latitude,
                    "longitude": r.longitude,
                    "use": r.use,
                    "digital_modes": [m.strip() for m in r.digital_modes.split(",")] if r.digital_modes else [],
                    "linked_nodes": r.linked_nodes,
                    "last_synced": r.last_synced.isoformat() if r.last_synced else None,
                }
                for r in repeaters
            ],
        }
kind: ConfigMap
metadata:
  creationTimestamp: null
  name: sdr-viewer-api-overrides
  namespace: sdr-research
