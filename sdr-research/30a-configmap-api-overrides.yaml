apiVersion: v1
data:
  audio.py: |
    import numpy as np
    import librosa
    import librosa.display
    import matplotlib
    matplotlib.use("Agg")
    import matplotlib.pyplot as plt


    def generate_waveform_peaks(audio_path: str, num_peaks: int = 1000) -> dict:
        """Generate waveform peaks for visualization.

        Returns a dict with:
        - peaks: list of [min, max] values per segment
        - duration: total duration in seconds
        - sample_rate: original sample rate
        """
        # Load audio
        y, sr = librosa.load(audio_path, sr=None, mono=True)
        duration = len(y) / sr

        # Calculate samples per peak
        samples_per_peak = max(1, len(y) // num_peaks)

        peaks = []
        for i in range(0, len(y), samples_per_peak):
            segment = y[i : i + samples_per_peak]
            if len(segment) > 0:
                peaks.append([float(np.min(segment)), float(np.max(segment))])

        return {
            "peaks": peaks,
            "duration": duration,
            "sample_rate": sr,
            "num_samples": len(y),
        }


    def generate_spectrogram(
        audio_path: str,
        output_path: str,
        figsize: tuple = (12, 4),
        max_seconds: int = 90,
    ):
        """Generate spectrogram image and save to file."""
        # Bound decode duration to avoid huge memory/latency on very long captures.
        y, sr = librosa.load(audio_path, sr=8000, mono=True, duration=max_seconds)

        # Compute spectrogram
        hop_length = max(256, len(y) // 2000) if len(y) else 256
        D = librosa.amplitude_to_db(
            np.abs(librosa.stft(y, n_fft=512, hop_length=hop_length)),
            ref=np.max,
        )

        # Create figure
        fig, ax = plt.subplots(figsize=figsize)
        img = librosa.display.specshow(D, sr=sr, x_axis="time", y_axis="hz", ax=ax, cmap="magma")
        ax.set_xlabel("Time (s)")
        ax.set_ylabel("Frequency (Hz)")
        fig.colorbar(img, ax=ax, format="%+2.0f dB")

        # Save
        plt.tight_layout()
        plt.savefig(output_path, dpi=100, bbox_inches="tight", pad_inches=0.1)
        plt.close(fig)


    def get_time_segments(transcript: str, duration: float) -> list:
        """Estimate time segments for transcript words.

        Since Whisper doesn't always provide word-level timestamps in our setup,
        we estimate based on character position and total duration.
        """
        if not transcript:
            return []

        words = transcript.split()
        total_chars = len(transcript)
        if total_chars == 0:
            return []

        segments = []
        char_pos = 0
        for word in words:
            start_time = (char_pos / total_chars) * duration
            char_pos += len(word) + 1  # +1 for space
            end_time = (char_pos / total_chars) * duration
            segments.append({
                "word": word,
                "start": start_time,
                "end": end_time,
            })

        return segments
  files.py: |
    import os
    from datetime import datetime, timedelta
    from typing import Optional, List

    from fastapi import APIRouter, Depends, HTTPException, Query, Request
    from fastapi.responses import StreamingResponse, FileResponse
    from pydantic import BaseModel
    from sqlalchemy import desc, func, or_, text as sql_text
    from sqlalchemy.orm import Session

    from ..database import get_db
    from ..models import CallsignInfo, Recording, Repeater
    from ..services.tagging import extract_callsign_tags, normalize_callsign, parse_ai_tags

    router = APIRouter()


    class UpdateRecordingRequest(BaseModel):
        ai_tags: Optional[List[str]] = None
        transcript: Optional[str] = None


    class BulkDeleteRequest(BaseModel):
        ids: List[int]


    class BulkDeleteFilteredRequest(BaseModel):
        mode: Optional[str] = None
        frequency_min: Optional[float] = None
        frequency_max: Optional[float] = None
        q: Optional[str] = None
        callsign: Optional[str] = None
        date_from: Optional[datetime] = None
        date_to: Optional[datetime] = None
        duration_min: Optional[float] = None
        duration_max: Optional[float] = None
        has_transcript: Optional[bool] = None
        dry_run: bool = False


    def _validate_mode(mode: Optional[str]):
        if mode is not None and mode not in {"cw", "voice"}:
            raise HTTPException(status_code=422, detail="mode must be 'cw' or 'voice'")


    def _validate_duration_bounds(duration_min: Optional[float], duration_max: Optional[float]):
        if (
            duration_min is not None
            and duration_max is not None
            and duration_min > duration_max
        ):
            raise HTTPException(
                status_code=422,
                detail="duration_min cannot be greater than duration_max",
            )


    def _all_tags(callsign_tags: List[str], ai_tags: List[str]) -> List[str]:
        return list(dict.fromkeys(callsign_tags + ai_tags))


    def _apply_recording_filters(
        query,
        mode: Optional[str] = None,
        frequency_min: Optional[float] = None,
        frequency_max: Optional[float] = None,
        q: Optional[str] = None,
        callsign: Optional[str] = None,
        date_from: Optional[datetime] = None,
        date_to: Optional[datetime] = None,
        duration_min: Optional[float] = None,
        duration_max: Optional[float] = None,
        has_transcript: Optional[bool] = None,
    ):
        if mode:
            query = query.filter(Recording.mode == mode)
        if frequency_min is not None:
            query = query.filter(Recording.frequency_hz >= frequency_min)
        if frequency_max is not None:
            query = query.filter(Recording.frequency_hz <= frequency_max)

        if q:
            cleaned_q = q.strip()
            if cleaned_q:
                search_query = func.plainto_tsquery("english", cleaned_q)
                query = query.filter(
                    or_(
                        Recording.search_vector.op("@@")(search_query),
                        Recording.transcript.ilike(f"%{cleaned_q}%"),
                    )
                )

        if callsign:
            normalized_callsign = normalize_callsign(callsign)
            if normalized_callsign:
                normalized_transcript = func.regexp_replace(
                    func.upper(func.coalesce(Recording.transcript, "")),
                    "[^A-Z0-9]+",
                    "",
                    "g",
                )
                query = query.filter(normalized_transcript.ilike(f"%{normalized_callsign}%"))

        if date_from:
            query = query.filter(Recording.timestamp >= date_from)
        if date_to:
            query = query.filter(Recording.timestamp <= date_to)
        if duration_min is not None:
            query = query.filter(Recording.duration_seconds >= duration_min)
        if duration_max is not None:
            query = query.filter(Recording.duration_seconds <= duration_max)

        transcript_len = func.length(func.trim(func.coalesce(Recording.transcript, "")))
        if has_transcript is True:
            query = query.filter(transcript_len > 0)
        elif has_transcript is False:
            query = query.filter(transcript_len == 0)

        return query


    def _apply_tag_filter(query, tag: Optional[str]):
        """Filter by an AI or callsign tag substring present in the JSON tags list."""
        if not tag:
            return query
        cleaned = tag.strip().lower()
        if not cleaned:
            return query
        # ai_tags is a JSON-encoded list like ["repeater","W3RDW","voice"]
        # Match tag as a JSON string element, case-insensitive.
        query = query.filter(Recording.ai_tags.ilike(f'%"{cleaned}"%'))
        return query


    @router.get("/browse")
    async def browse_files(
        mode: Optional[str] = Query(None, pattern="^(cw|voice|aprs)$"),
        frequency_min: Optional[float] = None,
        frequency_max: Optional[float] = None,
        q: Optional[str] = Query(None, min_length=1),
        callsign: Optional[str] = Query(None, min_length=1, max_length=20),
        tag: Optional[str] = Query(None, min_length=1, max_length=50),
        repeater: Optional[str] = Query(None, min_length=1, max_length=20),
        date_from: Optional[datetime] = None,
        date_to: Optional[datetime] = None,
        duration_min: Optional[float] = Query(None, ge=0),
        duration_max: Optional[float] = Query(None, ge=0),
        has_transcript: Optional[bool] = None,
        page: int = Query(1, ge=1),
        limit: int = Query(50, ge=1, le=200),
        db: Session = Depends(get_db),
    ):
        """List recordings with optional filters."""
        _validate_duration_bounds(duration_min, duration_max)

        query = _apply_recording_filters(
            db.query(Recording),
            mode=mode,
            frequency_min=frequency_min,
            frequency_max=frequency_max,
            q=q,
            callsign=callsign,
            date_from=date_from,
            date_to=date_to,
            duration_min=duration_min,
            duration_max=duration_max,
            has_transcript=has_transcript,
        )
        query = _apply_tag_filter(query, tag)

        if repeater:
            query = (
                query
                .join(Repeater, Recording.repeater_id == Repeater.id)
                .filter(Repeater.callsign.ilike(f"%{repeater.strip().upper()}%"))
            )

        total = query.count()
        recordings = (
            query.order_by(desc(Recording.timestamp))
            .offset((page - 1) * limit)
            .limit(limit)
            .all()
        )

        return {
            "total": total,
            "page": page,
            "limit": limit,
            "items": [
                {
                    "id": r.id,
                    "filename": r.filename,
                    "mode": r.mode,
                    "frequency_hz": r.frequency_hz,
                    "frequency_label": r.frequency_label,
                    "timestamp": r.timestamp.isoformat() if r.timestamp else None,
                    "duration_seconds": r.duration_seconds,
                    "has_transcript": bool((r.transcript or "").strip()),
                    "callsign_tags": (callsign_tags := extract_callsign_tags(r.transcript)),
                    "ai_tags": (ai_tags := parse_ai_tags(r.ai_tags)),
                    "tags": _all_tags(callsign_tags, ai_tags),
                }
                for r in recordings
            ],
        }


    @router.get("/callsign/{callsign}")
    async def get_callsign_info(callsign: str, db: Session = Depends(get_db)):
        """Return operator info and recording statistics for a callsign."""
        cs = callsign.strip().upper()
        info = db.query(CallsignInfo).filter(CallsignInfo.callsign == cs).first()
        normalized_cs = normalize_callsign(cs)
        total = 0
        first_heard = None
        last_heard = None
        total_airtime = 0.0
        if normalized_cs:
            normalized_transcript = func.regexp_replace(
                func.upper(func.coalesce(Recording.transcript, "")),
                "[^A-Z0-9]+",
                "",
                "g",
            )
            base_q = db.query(Recording).filter(
                normalized_transcript.ilike(f"%{normalized_cs}%")
            )
            total = base_q.count()
            first_heard = (
                db.query(func.min(Recording.timestamp))
                .filter(normalized_transcript.ilike(f"%{normalized_cs}%"))
                .scalar()
            )
            last_heard = (
                db.query(func.max(Recording.timestamp))
                .filter(normalized_transcript.ilike(f"%{normalized_cs}%"))
                .scalar()
            )
            total_airtime = (
                db.query(func.sum(Recording.duration_seconds))
                .filter(normalized_transcript.ilike(f"%{normalized_cs}%"))
                .scalar()
            ) or 0.0
        return {
            "callsign": cs,
            "operator": {
                "name": info.name,
                "qth_city": info.qth_city,
                "qth_state": info.qth_state,
                "license_class": info.license_class,
                "grid": info.grid,
            } if info else None,
            "total_recordings": total,
            "first_heard": first_heard.isoformat() if first_heard else None,
            "last_heard": last_heard.isoformat() if last_heard else None,
            "total_airtime_seconds": round(total_airtime, 1),
        }


    @router.get("/{file_id}")
    async def get_file(file_id: int, db: Session = Depends(get_db)):
        """Get file metadata and transcript."""
        recording = db.query(Recording).filter(Recording.id == file_id).first()
        if not recording:
            raise HTTPException(status_code=404, detail="Recording not found")

        callsign_tags = extract_callsign_tags(recording.transcript)
        ai_tags = parse_ai_tags(recording.ai_tags)

        # Repeater info
        repeater_info = None
        if recording.repeater_id:
            rep = db.query(Repeater).filter(Repeater.id == recording.repeater_id).first()
            if rep:
                repeater_info = {
                    "id": rep.id,
                    "callsign": rep.callsign,
                    "location": rep.location,
                    "county": rep.county,
                    "state": rep.state,
                    "frequency_hz": rep.frequency_hz,
                    "input_hz": rep.input_hz,
                    "pl_tone": rep.pl_tone,
                    "digital_modes": [m.strip() for m in rep.digital_modes.split(",")] if rep.digital_modes else [],
                    "linked_nodes": rep.linked_nodes,
                    "use": rep.use,
                }

        # Operator info for callsigns heard in transcript
        operators = []
        for cs in callsign_tags[:5]:
            info = db.query(CallsignInfo).filter(CallsignInfo.callsign == cs).first()
            if info:
                operators.append({
                    "callsign": info.callsign,
                    "name": info.name,
                    "qth_city": info.qth_city,
                    "qth_state": info.qth_state,
                    "license_class": info.license_class,
                    "grid": info.grid,
                })

        return {
            "id": recording.id,
            "filename": recording.filename,
            "mode": recording.mode,
            "frequency_hz": recording.frequency_hz,
            "timestamp": recording.timestamp.isoformat() if recording.timestamp else None,
            "duration_seconds": recording.duration_seconds,
            "transcript": recording.transcript,
            "frequency_label": recording.frequency_label,
            "callsign_tags": callsign_tags,
            "ai_tags": ai_tags,
            "tags": _all_tags(callsign_tags, ai_tags),
            "has_waveform": recording.waveform_cached is not None,
            "has_spectrogram": recording.spectrogram_cached is not None,
            "repeater": repeater_info,
            "operators": operators,
        }


    @router.get("/{file_id}/neighbors")
    async def get_file_neighbors(file_id: int, db: Session = Depends(get_db)):
        """Get previous and next recording IDs by timestamp."""
        recording = db.query(Recording).filter(Recording.id == file_id).first()
        if not recording:
            raise HTTPException(status_code=404, detail="Recording not found")
        ts = recording.timestamp
        if ts:
            prev_rec = (
                db.query(Recording.id)
                .filter(Recording.timestamp < ts)
                .order_by(desc(Recording.timestamp))
                .first()
            )
            next_rec = (
                db.query(Recording.id)
                .filter(Recording.timestamp > ts)
                .order_by(Recording.timestamp)
                .first()
            )
        else:
            prev_rec = (
                db.query(Recording.id)
                .filter(Recording.id < file_id)
                .order_by(desc(Recording.id))
                .first()
            )
            next_rec = (
                db.query(Recording.id)
                .filter(Recording.id > file_id)
                .order_by(Recording.id)
                .first()
            )
        return {
            "prev_id": prev_rec[0] if prev_rec else None,
            "next_id": next_rec[0] if next_rec else None,
        }


    @router.get("/{file_id}/related")
    async def get_related_recordings(file_id: int, db: Session = Depends(get_db)):
        """Get nearby recordings on the same frequency (±10 kHz, ±1 hour)."""
        recording = db.query(Recording).filter(Recording.id == file_id).first()
        if not recording:
            raise HTTPException(status_code=404, detail="Recording not found")
        items = []
        if recording.frequency_hz is not None and recording.timestamp is not None:
            ts_min = recording.timestamp - timedelta(hours=1)
            ts_max = recording.timestamp + timedelta(hours=1)
            freq_min = recording.frequency_hz - 10000
            freq_max = recording.frequency_hz + 10000
            rows = (
                db.query(Recording)
                .filter(
                    Recording.id != recording.id,
                    Recording.frequency_hz.between(freq_min, freq_max),
                    Recording.timestamp.between(ts_min, ts_max),
                )
                .order_by(desc(Recording.timestamp))
                .limit(10)
                .all()
            )
            for r in rows:
                cs_tags = extract_callsign_tags(r.transcript)
                ai_tags_list = parse_ai_tags(r.ai_tags)
                items.append({
                    "id": r.id,
                    "filename": r.filename,
                    "mode": r.mode,
                    "frequency_hz": r.frequency_hz,
                    "frequency_label": r.frequency_label,
                    "timestamp": r.timestamp.isoformat() if r.timestamp else None,
                    "duration_seconds": r.duration_seconds,
                    "has_transcript": bool((r.transcript or "").strip()),
                    "callsign_tags": cs_tags,
                    "ai_tags": ai_tags_list,
                    "tags": _all_tags(cs_tags, ai_tags_list),
                })
        return {"items": items, "count": len(items)}


    @router.patch("/{file_id}")
    async def update_recording(
        file_id: int, body: UpdateRecordingRequest, db: Session = Depends(get_db)
    ):
        """Update mutable fields on a recording (ai_tags and/or transcript)."""
        recording = db.query(Recording).filter(Recording.id == file_id).first()
        if not recording:
            raise HTTPException(status_code=404, detail="Recording not found")

        if body.ai_tags is not None:
            from ..services.tagging import dump_ai_tags
            recording.ai_tags = dump_ai_tags(body.ai_tags)

        if body.transcript is not None:
            new_transcript = body.transcript.strip() or None
            recording.transcript = new_transcript
            db.execute(
                sql_text("UPDATE recordings SET search_vector = NULL WHERE id = :id"),
                {"id": recording.id},
            )
            if new_transcript:
                db.execute(
                    sql_text(
                        "UPDATE recordings SET search_vector = to_tsvector('english', :t) WHERE id = :id"
                    ),
                    {"t": new_transcript, "id": recording.id},
                )

        db.commit()
        ai_tags = parse_ai_tags(recording.ai_tags)
        callsign_tags = extract_callsign_tags(recording.transcript)
        return {
            "id": recording.id,
            "ai_tags": ai_tags,
            "callsign_tags": callsign_tags,
            "tags": _all_tags(callsign_tags, ai_tags),
            "transcript": recording.transcript,
        }


    @router.post("/{file_id}/retag")
    async def retag_recording(file_id: int, db: Session = Depends(get_db)):
        """Clear AI tags so the indexer re-runs tagging on the next cycle."""
        recording = db.query(Recording).filter(Recording.id == file_id).first()
        if not recording:
            raise HTTPException(status_code=404, detail="Recording not found")
        recording.ai_tags = None
        db.commit()
        return {"status": "ok", "message": "AI tags cleared — will be regenerated on next indexer cycle"}


    @router.post("/{file_id}/retranscribe")
    async def retranscribe_recording(file_id: int, db: Session = Depends(get_db)):
        """Clear transcript and delete text file so Whisper re-transcribes on next cycle."""
        recording = db.query(Recording).filter(Recording.id == file_id).first()
        if not recording:
            raise HTTPException(status_code=404, detail="Recording not found")
        if not recording.audio_path:
            raise HTTPException(status_code=400, detail="No audio file — cannot re-transcribe")
        if recording.text_path and os.path.exists(recording.text_path):
            try:
                os.remove(recording.text_path)
            except OSError:
                pass
        recording.transcript = None
        recording.text_path = None
        recording.ai_tags = None
        db.execute(sql_text("UPDATE recordings SET search_vector = NULL WHERE id = :id"), {"id": recording.id})
        db.commit()
        return {"status": "ok", "message": "Transcript cleared — Whisper will re-transcribe on next cycle"}


    @router.get("/{file_id}/stream")
    async def stream_file(file_id: int, request: Request, db: Session = Depends(get_db)):
        """Stream audio file with Range request support."""
        recording = db.query(Recording).filter(Recording.id == file_id).first()
        if not recording:
            raise HTTPException(status_code=404, detail="Recording not found")

        audio_path = recording.audio_path
        if not os.path.exists(audio_path):
            raise HTTPException(status_code=404, detail="Audio file not found")

        file_size = os.path.getsize(audio_path)

        # Handle Range requests for seeking
        range_header = request.headers.get("Range")
        if range_header:
            try:
                range_spec = range_header.replace("bytes=", "")
                start, end = range_spec.split("-")
                start = int(start) if start else 0
                end = int(end) if end else file_size - 1
                end = min(end, file_size - 1)

                content_length = end - start + 1

                def iter_file():
                    with open(audio_path, "rb") as f:
                        f.seek(start)
                        remaining = content_length
                        while remaining > 0:
                            chunk_size = min(8192, remaining)
                            data = f.read(chunk_size)
                            if not data:
                                break
                            remaining -= len(data)
                            yield data

                return StreamingResponse(
                    iter_file(),
                    status_code=206,
                    media_type="audio/wav",
                    headers={
                        "Content-Range": f"bytes {start}-{end}/{file_size}",
                        "Content-Length": str(content_length),
                        "Accept-Ranges": "bytes",
                    },
                )
            except (ValueError, IndexError):
                pass

        # Full file response
        return FileResponse(
            audio_path,
            media_type="audio/wav",
            headers={"Accept-Ranges": "bytes"},
        )


    def _delete_recording_files(recording: Recording):
        """Remove audio, text, and cache files for a recording."""
        for path in [
            recording.audio_path,
            recording.text_path,
            recording.waveform_cached,
            recording.spectrogram_cached,
        ]:
            if path:
                try:
                    os.remove(path)
                except OSError:
                    pass


    @router.delete("/{file_id}")
    async def delete_file(file_id: int, db: Session = Depends(get_db)):
        """Delete a single recording and its files."""
        recording = db.query(Recording).filter(Recording.id == file_id).first()
        if not recording:
            raise HTTPException(status_code=404, detail="Recording not found")

        _delete_recording_files(recording)
        db.delete(recording)
        db.commit()
        return {"deleted": 1}


    @router.post("/bulk-delete")
    async def bulk_delete_files(body: BulkDeleteRequest, db: Session = Depends(get_db)):
        """Delete multiple recordings by ID."""
        recordings = db.query(Recording).filter(Recording.id.in_(body.ids)).all()
        count = 0
        for recording in recordings:
            _delete_recording_files(recording)
            db.delete(recording)
            count += 1
        db.commit()
        return {"deleted": count}


    @router.post("/bulk-delete-filtered")
    async def bulk_delete_filtered_files(
        body: BulkDeleteFilteredRequest,
        db: Session = Depends(get_db),
    ):
        """Delete all recordings matching the provided filter criteria."""
        _validate_mode(body.mode)
        _validate_duration_bounds(body.duration_min, body.duration_max)

        has_any_filter = any(
            [
                body.mode is not None,
                body.frequency_min is not None,
                body.frequency_max is not None,
                body.q is not None,
                body.callsign is not None,
                body.date_from is not None,
                body.date_to is not None,
                body.duration_min is not None,
                body.duration_max is not None,
                body.has_transcript is not None,
            ]
        )
        if not has_any_filter:
            raise HTTPException(
                status_code=400,
                detail="At least one filter is required for bulk-delete-filtered",
            )

        query = _apply_recording_filters(
            db.query(Recording),
            mode=body.mode,
            frequency_min=body.frequency_min,
            frequency_max=body.frequency_max,
            q=body.q,
            callsign=body.callsign,
            date_from=body.date_from,
            date_to=body.date_to,
            duration_min=body.duration_min,
            duration_max=body.duration_max,
            has_transcript=body.has_transcript,
        )
        recordings = query.all()
        matched = len(recordings)

        if body.dry_run:
            return {"matched": matched, "deleted": 0}

        for recording in recordings:
            _delete_recording_files(recording)
            db.delete(recording)
        db.commit()

        return {"matched": matched, "deleted": matched}
  search.py: |
    from typing import Optional, List

    from fastapi import APIRouter, Depends, Query
    from sqlalchemy import func, text
    from sqlalchemy.orm import Session

    from ..database import get_db
    from ..models import Recording
    from ..services.tagging import extract_callsign_tags, normalize_callsign, parse_ai_tags

    router = APIRouter()


    @router.get("/text")
    async def search_text(
        q: str = Query(..., min_length=1),
        mode: Optional[str] = Query(None, pattern="^(cw|voice|aprs)$"),
        frequency_min: Optional[float] = None,
        frequency_max: Optional[float] = None,
        callsign: Optional[str] = Query(None, min_length=1, max_length=20),
        page: int = Query(1, ge=1),
        limit: int = Query(50, ge=1, le=200),
        db: Session = Depends(get_db),
    ):
        """Full-text search on transcripts."""
        search_query = func.plainto_tsquery("english", q)

        query = db.query(
            Recording,
            func.ts_rank(Recording.search_vector, search_query).label("rank"),
            func.ts_headline(
                "english",
                Recording.transcript,
                search_query,
                "StartSel=<mark>, StopSel=</mark>, MaxWords=50, MinWords=20",
            ).label("headline"),
        ).filter(Recording.search_vector.op("@@")(search_query))

        if mode:
            query = query.filter(Recording.mode == mode)
        if frequency_min is not None:
            query = query.filter(Recording.frequency_hz >= frequency_min)
        if frequency_max is not None:
            query = query.filter(Recording.frequency_hz <= frequency_max)

        if callsign:
            normalized_callsign = normalize_callsign(callsign)
            if normalized_callsign:
                normalized_transcript = func.regexp_replace(
                    func.upper(func.coalesce(Recording.transcript, "")),
                    "[^A-Z0-9]+",
                    "",
                    "g",
                )
                query = query.filter(normalized_transcript.ilike(f"%{normalized_callsign}%"))

        total = query.count()
        results = query.order_by(text("rank DESC")).offset((page - 1) * limit).limit(limit).all()

        return {
            "query": q,
            "total": total,
            "page": page,
            "limit": limit,
            "items": [
                {
                    "id": r.Recording.id,
                    "filename": r.Recording.filename,
                    "mode": r.Recording.mode,
                    "frequency_hz": r.Recording.frequency_hz,
                    "frequency_label": r.Recording.frequency_label,
                    "timestamp": r.Recording.timestamp.isoformat() if r.Recording.timestamp else None,
                    "headline": r.headline,
                    "rank": float(r.rank),
                    "callsign_tags": (callsign_tags := extract_callsign_tags(r.Recording.transcript)),
                    "ai_tags": (ai_tags := parse_ai_tags(r.Recording.ai_tags)),
                    "tags": list(dict.fromkeys(callsign_tags + ai_tags)),
                }
                for r in results
            ],
        }
  tagging.py: |
    import json
    import os
    import re
    from typing import List, Optional


    CALLSIGN_PATTERN = re.compile(r"\b(?:[AKNW][A-Z]?\d[A-Z]{1,3})\b", re.IGNORECASE)
    NO_SPEECH_PATTERN = re.compile(
        r"^\s*(?:"
        r"\[?\s*no speech detected\s*\]?"
        r"|\[?\s*recording too long for auto-transcribe\s*\]?"
        r"|[^\w\s]+"
        r"|thanks?\s+(?:for\s+)?(?:watching|listening)\.?"
        r"|(?:thank\s+you|bye|goodbye|see\s+you)\.?"
        r"|\[(?:music|applause|laughter|silence|noise|inaudible)[^\]]*\]"
        r")\s*$",
        re.IGNORECASE,
    )

    PHONETIC_MAP = {
        "ALFA": "A", "ALPHA": "A", "BRAVO": "B", "CHARLIE": "C", "DELTA": "D",
        "ECHO": "E", "FOXTROT": "F", "GOLF": "G", "HOTEL": "H", "INDIA": "I",
        "JULIETT": "J", "JULIET": "J", "KILO": "K", "LIMA": "L", "MIKE": "M",
        "NOVEMBER": "N", "OSCAR": "O", "PAPA": "P", "QUEBEC": "Q", "ROMEO": "R",
        "SIERRA": "S", "TANGO": "T", "UNIFORM": "U", "VICTOR": "V", "WHISKEY": "W",
        "XRAY": "X", "X-RAY": "X", "YANKEE": "Y", "ZULU": "Z",
        "ZERO": "0", "OH": "0", "ONE": "1", "TWO": "2", "TOO": "2",
        "THREE": "3", "TREE": "3", "FOUR": "4", "FIVE": "5", "SIX": "6",
        "SEVEN": "7", "EIGHT": "8", "NINE": "9", "NINER": "9",
    }


    def normalize_callsign(value: str) -> str:
        return re.sub(r"[^A-Z0-9]", "", value.upper())


    def callsign_hints() -> List[str]:
        raw = os.getenv("CALLSIGN_HINTS", "w3rdw")
        hints: List[str] = []
        for token in raw.split(","):
            normalized = normalize_callsign(token.strip())
            if normalized:
                hints.append(normalized)
        return hints


    def phonetic_compact(text: str) -> str:
        tokens = re.findall(r"[A-Z0-9-]+", text.upper())
        compact: List[str] = []
        for token in tokens:
            mapped = PHONETIC_MAP.get(token)
            if mapped:
                compact.append(mapped)
                continue
            if len(token) == 1 and token.isalnum():
                compact.append(token)
                continue
            if token.isdigit():
                compact.append(token)
        return "".join(compact)


    def extract_callsign_tags(transcript: Optional[str]) -> List[str]:
        if not transcript:
            return []
        tags: List[str] = []
        seen = set()
        for match in CALLSIGN_PATTERN.finditer(transcript):
            tag = normalize_callsign(match.group(0))
            if tag and tag not in seen:
                seen.add(tag)
                tags.append(tag)
        normalized_text = normalize_callsign(transcript)
        phonetic_text = phonetic_compact(transcript)
        for hint in callsign_hints():
            if (hint in normalized_text or hint in phonetic_text) and hint not in seen:
                seen.add(hint)
                tags.append(hint)
        return tags


    def is_no_speech_transcript(transcript: Optional[str]) -> bool:
        if not transcript:
            return False
        return NO_SPEECH_PATTERN.match(transcript) is not None


    def parse_ai_tags(raw: Optional[str]) -> List[str]:
        if not raw:
            return []
        try:
            value = json.loads(raw)
        except json.JSONDecodeError:
            return []
        if not isinstance(value, list):
            return []
        tags: List[str] = []
        seen = set()
        for item in value:
            if not isinstance(item, str):
                continue
            cleaned = item.strip()
            if cleaned and cleaned not in seen:
                seen.add(cleaned)
                tags.append(cleaned)
        return tags


    def dump_ai_tags(tags: List[str]) -> Optional[str]:
        cleaned: List[str] = []
        seen = set()
        for tag in tags:
            normalized = tag.strip()
            if normalized and normalized not in seen:
                seen.add(normalized)
                cleaned.append(normalized)
        if not cleaned:
            return None
        return json.dumps(cleaned)
  known_freqs.py: |
    """
    Static lookup table of well-known frequencies.

    Frequencies are stored in Hz with a match tolerance. The first match wins,
    so list more specific entries (narrow-band) before broader band ranges.
    Edit this configmap to add or adjust entries without rebuilding the image.
    """

    from typing import Optional


    # (center_hz, tolerance_hz, label)
    _KNOWN: list[tuple[float, float, str]] = [
        # ── NOAA Weather Radio ────────────────────────────────────────────────
        (162_400_000, 3_000, "NOAA WX-1"),
        (162_425_000, 3_000, "NOAA WX-2"),
        (162_450_000, 3_000, "NOAA WX-3"),
        (162_475_000, 3_000, "NOAA WX-4"),
        (162_500_000, 3_000, "NOAA WX-5"),
        (162_525_000, 3_000, "NOAA WX-6"),
        (162_550_000, 3_000, "NOAA WX-7"),

        # ── Amateur — 2m ──────────────────────────────────────────────────────
        (144_200_000, 3_000, "2m SSB Calling"),
        (144_390_000, 3_000, "APRS 2m"),
        (146_520_000, 5_000, "2m National Simplex Calling"),
        (146_580_000, 5_000, "2m FM Simplex"),
        (147_555_000, 5_000, "2m FM Simplex"),

        # ── Amateur — 70cm ────────────────────────────────────────────────────
        (432_100_000, 3_000, "70cm SSB Calling"),
        (433_920_000, 5_000, "70cm FM Simplex"),
        (446_000_000, 5_000, "70cm National Simplex Calling"),
        (446_500_000, 5_000, "70cm FM Simplex"),

        # ── Amateur — 1.25m ───────────────────────────────────────────────────
        (223_500_000, 5_000, "1.25m National Simplex Calling"),

        # ── ISS ───────────────────────────────────────────────────────────────
        (145_800_000, 5_000, "ISS Voice Downlink"),
        (437_550_000, 5_000, "ISS Packet"),
        (145_825_000, 3_000, "ISS APRS"),

        # ── Aviation ──────────────────────────────────────────────────────────
        (121_500_000, 5_000, "Aviation Distress (Guard)"),
        (122_750_000, 5_000, "Aviation Multicom"),
        (123_025_000, 5_000, "Aviation Unicom"),
        (123_450_000, 5_000, "Aviation Air-to-Air"),

        # ── Marine VHF ───────────────────────────────────────────────────────
        (156_800_000, 5_000, "Marine Ch 16 (Distress/Calling)"),
        (156_300_000, 5_000, "Marine Ch 6 (Safety)"),
        (157_050_000, 5_000, "Marine Ch 22A (USCG Working)"),

        # ── FRS/GMRS simplex ─────────────────────────────────────────────────
        (462_562_500, 5_000, "FRS/GMRS Ch 1"),
        (462_587_500, 5_000, "FRS/GMRS Ch 2"),
        (462_612_500, 5_000, "FRS/GMRS Ch 3"),
        (462_637_500, 5_000, "FRS/GMRS Ch 4"),
        (462_662_500, 5_000, "FRS/GMRS Ch 5"),
        (462_687_500, 5_000, "FRS/GMRS Ch 6"),
        (462_712_500, 5_000, "FRS/GMRS Ch 7"),
        (467_562_500, 5_000, "FRS Ch 8"),
        (467_587_500, 5_000, "FRS Ch 9"),
        (467_612_500, 5_000, "FRS Ch 10"),
        (467_637_500, 5_000, "FRS Ch 11"),
        (467_662_500, 5_000, "FRS Ch 12"),
        (467_687_500, 5_000, "FRS Ch 13"),
        (467_712_500, 5_000, "FRS Ch 14"),
        (462_550_000, 5_000, "GMRS Ch 15 (Calling)"),
        (462_575_000, 5_000, "GMRS Ch 16"),
        (462_600_000, 5_000, "GMRS Ch 17"),
        (462_625_000, 5_000, "GMRS Ch 18"),
        (462_650_000, 5_000, "GMRS Ch 19"),
        (462_675_000, 5_000, "GMRS Ch 20"),
        (462_700_000, 5_000, "GMRS Ch 21"),
        (462_725_000, 5_000, "GMRS Ch 22"),

        # ── MURS ─────────────────────────────────────────────────────────────
        (151_820_000, 5_000, "MURS Ch 1"),
        (151_880_000, 5_000, "MURS Ch 2"),
        (151_940_000, 5_000, "MURS Ch 3"),
        (154_570_000, 5_000, "MURS Ch 4"),
        (154_600_000, 5_000, "MURS Ch 5"),
    ]


    def lookup_known_freq(frequency_hz: float) -> Optional[str]:
        """Return a human label for a known frequency, or None if unrecognized."""
        for center, tolerance, label in _KNOWN:
            if abs(frequency_hz - center) <= tolerance:
                return label
        return None
  repeaters.py: |
    from typing import Optional

    from fastapi import APIRouter, Depends, Query
    from sqlalchemy import asc
    from sqlalchemy.orm import Session

    from ..database import get_db
    from ..models import Repeater

    router = APIRouter()


    @router.get("/station")
    async def get_station_center():
        """Return the configured station coordinates (used as map center)."""
        from ..config import settings
        return {
            "latitude": settings.repeaterbook_latitude,
            "longitude": settings.repeaterbook_longitude,
        }


    @router.get("")
    async def list_repeaters(
        state: Optional[str] = Query(None, min_length=2, max_length=50),
        callsign: Optional[str] = Query(None, min_length=1, max_length=20),
        frequency_min: Optional[float] = None,
        frequency_max: Optional[float] = None,
        digital_only: bool = False,
        digital_mode: Optional[str] = Query(None, min_length=1, max_length=30),
        page: int = Query(1, ge=1),
        limit: int = Query(100, ge=1, le=2000),
        db: Session = Depends(get_db),
    ):
        """Browse known repeaters from the local RepeaterBook cache."""
        query = db.query(Repeater)

        if state:
            query = query.filter(Repeater.state.ilike(f"%{state}%"))
        if callsign:
            query = query.filter(Repeater.callsign.ilike(f"%{callsign.upper()}%"))
        if frequency_min is not None:
            query = query.filter(Repeater.frequency_hz >= frequency_min)
        if frequency_max is not None:
            query = query.filter(Repeater.frequency_hz <= frequency_max)
        if digital_only or digital_mode:
            query = query.filter(
                Repeater.digital_modes.isnot(None),
                Repeater.digital_modes != "",
            )
        if digital_mode:
            query = query.filter(Repeater.digital_modes.ilike(f"%{digital_mode}%"))

        total = query.count()
        repeaters = (
            query.order_by(asc(Repeater.frequency_hz))
            .offset((page - 1) * limit)
            .limit(limit)
            .all()
        )

        return {
            "total": total,
            "page": page,
            "limit": limit,
            "items": [
                {
                    "id": r.id,
                    "callsign": r.callsign,
                    "frequency_hz": r.frequency_hz,
                    "input_hz": r.input_hz,
                    "pl_tone": r.pl_tone,
                    "location": r.location,
                    "county": r.county,
                    "state": r.state,
                    "latitude": r.latitude,
                    "longitude": r.longitude,
                    "use": r.use,
                    "digital_modes": [m.strip() for m in r.digital_modes.split(",")] if r.digital_modes else [],
                    "linked_nodes": r.linked_nodes,
                    "last_synced": r.last_synced.isoformat() if r.last_synced else None,
                }
                for r in repeaters
            ],
        }
  main.py: |
    import asyncio
    from contextlib import asynccontextmanager

    from fastapi import FastAPI
    from fastapi.middleware.cors import CORSMiddleware

    from .database import engine, Base
    from .routers import admin, aprs, files, repeaters, search, stats, waveform
    from .services.indexer import run_indexer
    from .services.repeater import run_repeater_sync


    @asynccontextmanager
    async def lifespan(app: FastAPI):
        Base.metadata.create_all(bind=engine)
        indexer_task = asyncio.create_task(run_indexer())
        repeater_task = asyncio.create_task(run_repeater_sync())
        yield
        for task in (indexer_task, repeater_task):
            task.cancel()
            try:
                await task
            except asyncio.CancelledError:
                pass


    app = FastAPI(
        title="SDR Viewer API",
        description="API for browsing and streaming SDR recordings",
        version="0.1.0",
        lifespan=lifespan,
    )

    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )

    app.include_router(files.router, prefix="/api/v1/files", tags=["files"])
    app.include_router(search.router, prefix="/api/v1/search", tags=["search"])
    app.include_router(waveform.router, prefix="/api/v1/waveform", tags=["waveform"])
    app.include_router(repeaters.router, prefix="/api/v1/repeaters", tags=["repeaters"])
    app.include_router(aprs.router, prefix="/api/v1/aprs", tags=["aprs"])
    app.include_router(stats.router, prefix="/api/v1/stats", tags=["stats"])
    app.include_router(admin.router, prefix="/api/v1/admin", tags=["admin"])


    @app.get("/api/v1/health")
    async def health():
        return {"status": "ok"}

  aprs.py: |
    import re
    from datetime import datetime, timedelta
    from typing import Optional

    from fastapi import APIRouter, Depends, Query
    from sqlalchemy import desc
    from sqlalchemy.orm import Session

    from ..database import get_db
    from ..models import Recording

    router = APIRouter()

    # Uncompressed APRS position: !DDMM.mmN/DDDMM.mmW  (symbol table char between)
    _POS_RE = re.compile(
        r"[!=@/\\](\d{2})(\d{2}\.\d+)([NS])(.)(\d{3})(\d{2}\.\d+)([EW])"
    )
    _SPEED_COURSE_RE = re.compile(r"(\d{3})/(\d{3})")
    _ALT_RE = re.compile(r"A=(\d{6})")
    # APRS weather field pattern: cDDD sSS gGGG tTTT [rRRR] [pPPP] [hHH] [bBBBBB]
    _WEATHER_RE = re.compile(
        r"c(\d{3}|\.\.\.)s(\d{3}|\.\.\.)g(\d{3}|\.\.\.)t(-?\d{3}|\.\.\.)(?:r(\d{3}|\.{3}))?(?:p(\d{3}|\.{3}))?(?:h(\d{2}|\.{2}))?(?:b(\d{5}|\.{5}))?"
    )


    def _parse_weather(payload: str):
        m = _WEATHER_RE.search(payload)
        if not m:
            return None
        def _v(s):
            if s is None or "." in s:
                return None
            return int(s)
        wd = _v(m.group(1))
        ws = _v(m.group(2))
        wg = _v(m.group(3))
        temp = _v(m.group(4))
        if ws is None and temp is None:
            return None  # not a real weather packet
        r1 = _v(m.group(5))
        r24 = _v(m.group(6))
        hum = _v(m.group(7))
        baro = _v(m.group(8))
        return {
            "wind_dir_deg": wd,
            "wind_speed_mph": ws,
            "wind_gust_mph": wg,
            "temp_f": temp,
            "rain_1h_in": round(r1 * 0.01, 2) if r1 is not None else None,
            "rain_24h_in": round(r24 * 0.01, 2) if r24 is not None else None,
            "humidity_pct": hum,
            "pressure_mbar": round(baro * 0.1, 1) if baro is not None else None,
        }


    def _parse_position(payload: str):
        m = _POS_RE.search(payload)
        if not m:
            return None
        lat_d, lat_m, lat_h, _, lon_d, lon_m, lon_h = m.groups()
        lat = float(lat_d) + float(lat_m) / 60
        if lat_h == "S":
            lat = -lat
        lon = float(lon_d) + float(lon_m) / 60
        if lon_h == "W":
            lon = -lon
        return round(lat, 6), round(lon, 6)


    def _parse_comment(payload: str) -> str:
        s = re.sub(r"^[!=@/\\]\d{4}\.\d+[NS].\d{5}\.\d+[EW].", "", payload)
        s = re.sub(r"^\d{3}/\d{3}", "", s)
        s = re.sub(r"/?A=\d+", "", s)
        return s.strip()


    def _parse_packet(line: str):
        if ">" not in line or ":" not in line:
            return None
        try:
            callsign = line.split(">")[0].strip()
            colon_idx = line.index(":")
            header = line[:colon_idx]
            payload = line[colon_idx + 1:]
            path_parts = header.split(",")
            path = ",".join(path_parts[1:]) if len(path_parts) > 1 else ""
            pos = _parse_position(payload)
            sc = _SPEED_COURSE_RE.search(payload[1:20] if len(payload) > 1 else "")
            speed_kt = int(sc.group(2)) if sc else None
            course = int(sc.group(1)) if sc else None
            alt_m = _ALT_RE.search(payload)
            altitude_ft = int(alt_m.group(1)) if alt_m else None
            comment = _parse_comment(payload)
            is_weather = len(payload) > 0 and payload[0] == "_"
            weather = _parse_weather(payload) if is_weather else None
            return {
                "callsign": callsign,
                "path": path,
                "latitude": pos[0] if pos else None,
                "longitude": pos[1] if pos else None,
                "speed_kt": speed_kt,
                "course": course,
                "altitude_ft": altitude_ft,
                "comment": comment,
                "packet": line.strip(),
                "is_weather": is_weather,
                "weather": weather,
            }
        except Exception:
            return None


    @router.get("/stations")
    def get_aprs_stations(
        hours: int = Query(24, ge=1, le=168),
        db: Session = Depends(get_db),
    ):
        since = datetime.utcnow() - timedelta(hours=hours)
        rows = (
            db.query(Recording)
            .filter(
                Recording.mode == "aprs",
                Recording.transcript.isnot(None),
                Recording.timestamp >= since,
            )
            .order_by(desc(Recording.timestamp))
            .limit(2000)
            .all()
        )
        stations: dict = {}
        for rec in rows:
            for line in (rec.transcript or "").splitlines():
                p = _parse_packet(line)
                if not p:
                    continue
                cs = p["callsign"]
                if cs not in stations:
                    stations[cs] = {
                        **p,
                        "last_heard": rec.timestamp.isoformat() if rec.timestamp else None,
                        "frequency_hz": rec.frequency_hz,
                    }
                elif p["latitude"] is not None and stations[cs]["latitude"] is None:
                    stations[cs].update({
                        **p,
                        "last_heard": rec.timestamp.isoformat() if rec.timestamp else None,
                    })
        return {"stations": list(stations.values()), "hours": hours}


    @router.get("/tracks")
    def get_aprs_tracks(
        hours: int = Query(24, ge=1, le=168),
        db: Session = Depends(get_db),
    ):
        """Return per-callsign position tracks for drawing on the map."""
        since = datetime.utcnow() - timedelta(hours=hours)
        rows = (
            db.query(Recording)
            .filter(
                Recording.mode == "aprs",
                Recording.transcript.isnot(None),
                Recording.timestamp >= since,
            )
            .order_by(Recording.timestamp)
            .limit(5000)
            .all()
        )
        tracks: dict = {}
        for rec in rows:
            for line in (rec.transcript or "").splitlines():
                p = _parse_packet(line)
                if not p or p["latitude"] is None or p["longitude"] is None:
                    continue
                cs = p["callsign"]
                if cs not in tracks:
                    tracks[cs] = []
                tracks[cs].append({
                    "lat": p["latitude"],
                    "lon": p["longitude"],
                    "timestamp": rec.timestamp.isoformat() if rec.timestamp else None,
                })
        return {
            "tracks": [
                {"callsign": cs, "positions": positions}
                for cs, positions in tracks.items()
                if len(positions) >= 2
            ],
            "hours": hours,
        }


    @router.get("/packets")
    def get_aprs_packets(
        callsign: Optional[str] = None,
        hours: int = Query(24, ge=1, le=168),
        page: int = Query(1, ge=1),
        limit: int = Query(50, ge=1, le=200),
        db: Session = Depends(get_db),
    ):
        since = datetime.utcnow() - timedelta(hours=hours)
        q = db.query(Recording).filter(
            Recording.mode == "aprs",
            Recording.timestamp >= since,
        )
        if callsign:
            q = q.filter(Recording.transcript.ilike(f"%{callsign.upper()}%"))
        total = q.count()
        rows = (
            q.order_by(desc(Recording.timestamp))
            .offset((page - 1) * limit)
            .limit(limit)
            .all()
        )
        packets = []
        for rec in rows:
            for line in (rec.transcript or "").splitlines():
                p = _parse_packet(line)
                if p:
                    packets.append({
                        **p,
                        "id": rec.id,
                        "timestamp": rec.timestamp.isoformat() if rec.timestamp else None,
                        "frequency_hz": rec.frequency_hz,
                    })
        return {"packets": packets, "total": total, "page": page, "limit": limit}

  indexer.py: |
    import asyncio
    import json
    import os
    import re
    from datetime import datetime
    from pathlib import Path
    from urllib import error, request

    import librosa
    from sqlalchemy import text

    from ..config import settings
    from ..database import SessionLocal
    from ..models import Recording
    from .alerting import check_alerts
    from .hamdb import callsign_context_str, lookup_callsigns
    from .known_freqs import lookup_known_freq
    from .repeater import lookup_repeater, repeater_label, repeater_tags
    from .tagging import dump_ai_tags, extract_callsign_tags, is_no_speech_transcript


    CALLSIGN_TAG_PATTERN = re.compile(r"^[AKNW][A-Z]?\d[A-Z]{1,3}$", re.IGNORECASE)


    def ensure_recordings_schema(db):
        db.execute(text("ALTER TABLE recordings ADD COLUMN IF NOT EXISTS ai_tags TEXT"))
        db.execute(text("ALTER TABLE recordings ADD COLUMN IF NOT EXISTS repeater_id INTEGER"))
        db.execute(text("ALTER TABLE recordings ADD COLUMN IF NOT EXISTS frequency_label TEXT"))
        db.commit()


    def safe_unlink(path: str | None):
        if not path:
            return
        try:
            os.remove(path)
        except OSError:
            pass


    def prune_missing_recordings():
        db = SessionLocal()
        removed = 0
        try:
            ensure_recordings_schema(db)
            recordings = db.query(Recording).all()
            for recording in recordings:
                if recording.audio_path:
                    # Has audio: prune if file missing
                    if not os.path.exists(recording.audio_path):
                        db.delete(recording)
                        removed += 1
                else:
                    # No audio (e.g. APRS text-only): prune if text also missing
                    if not recording.text_path or not os.path.exists(recording.text_path):
                        db.delete(recording)
                        removed += 1
            if removed:
                db.commit()
                print(f"Pruned {removed} stale recordings (missing files)")
        except Exception as e:
            print(f"Prune error: {e}")
            db.rollback()
        finally:
            db.close()


    def parse_voice_filename(filename: str) -> dict:
        match = re.match(r"(\d+)_(\d+)\.wav$", filename)
        if match:
            freq_hz = float(match.group(1))
            timestamp = datetime.fromtimestamp(int(match.group(2)))
            return {"frequency_hz": freq_hz, "timestamp": timestamp}
        return {}


    def parse_cw_filename(filename: str) -> dict:
        match = re.match(r"cw_(\d+)_(\d+)\.wav$", filename)
        if match:
            freq_hz = float(match.group(1))
            timestamp = datetime.fromtimestamp(int(match.group(2)))
            return {"frequency_hz": freq_hz, "timestamp": timestamp}
        match = re.match(r"cw_(\d+)\.wav$", filename)
        if match:
            freq_hz = float(match.group(1))
            return {"frequency_hz": freq_hz}
        return {}


    def get_audio_duration(audio_path: str) -> float:
        try:
            duration = librosa.get_duration(path=audio_path)
            return duration
        except Exception:
            return 0.0


    def read_transcript(text_path: str) -> str | None:
        if os.path.exists(text_path):
            try:
                with open(text_path, "r") as f:
                    return f.read().strip()
            except Exception:
                pass
        return None


    def update_search_vector(db, recording_id: int, transcript: str):
        db.execute(
            text(
                "UPDATE recordings SET search_vector = to_tsvector('english', :transcript) WHERE id = :id"
            ),
            {"transcript": transcript, "id": recording_id},
        )
        db.commit()


    def sanitize_tag(tag: str) -> str | None:
        text_tag = tag.strip()
        if not text_tag:
            return None
        upper_tag = text_tag.upper()
        if CALLSIGN_TAG_PATTERN.match(upper_tag):
            return upper_tag
        lowered = text_tag.lower().replace("-", " ")
        lowered = re.sub(r"[^a-z0-9_ ]+", "", lowered)
        lowered = re.sub(r"\s+", "_", lowered).strip("_")
        if len(lowered) < 2 or len(lowered) > 48:
            return None
        return lowered


    def build_ollama_prompt(transcript, callsign_tags, frequency_label=None, repeater_info=None, operator_context=None):
        callsign_context = ", ".join(callsign_tags) if callsign_tags else "none"
        clipped = transcript.strip()
        if len(clipped) > 1600:
            clipped = clipped[:1600]
        context_lines = [f"Known callsigns: {callsign_context}"]
        if operator_context:
            context_lines.append(f"Operators:\n{operator_context}")
        if frequency_label:
            context_lines.append(f"Frequency: {frequency_label}")
        if repeater_info:
            context_lines.append(f"Repeater: {repeater_info}")
        context = "\n".join(context_lines)
        return (
            "You tag radio transcripts for later filtering. "
            "Return ONLY valid JSON with this exact schema: "
            "{\"tags\":[\"tag_one\",\"tag_two\"]}. "
            "Rules: 3 to 8 concise tags, no sentences, no duplicates, "
            "prefer lowercase underscore tags, include callsign tags when relevant.\n\n"
            f"{context}\n"
            f"Transcript:\n{clipped}"
        )


    def generate_ollama_tags(transcript, frequency_label=None, repeater_info=None, operator_context=None):
        if not settings.ollama_enabled:
            return []
        if not transcript or is_no_speech_transcript(transcript):
            return []
        callsign_tags = extract_callsign_tags(transcript)
        payload = {
            "model": settings.ollama_model,
            "stream": False,
            "format": "json",
            "options": {"temperature": 0.1},
            "prompt": build_ollama_prompt(
                transcript, callsign_tags,
                frequency_label=frequency_label,
                repeater_info=repeater_info,
                operator_context=operator_context,
            ),
        }
        endpoint = settings.ollama_url.rstrip("/") + "/api/generate"
        req = request.Request(
            endpoint,
            data=json.dumps(payload).encode("utf-8"),
            headers={"Content-Type": "application/json"},
            method="POST",
        )
        try:
            with request.urlopen(req, timeout=settings.ollama_timeout_seconds) as resp:
                body = resp.read().decode("utf-8", errors="ignore")
        except error.URLError as exc:
            print(f"Ollama request failed: {exc}")
            return callsign_tags
        except Exception as exc:
            print(f"Ollama request error: {exc}")
            return callsign_tags
        tags: list[str] = []
        seen = set()
        for tag in callsign_tags:
            cleaned = sanitize_tag(tag)
            if cleaned and cleaned not in seen:
                seen.add(cleaned)
                tags.append(cleaned)
        try:
            outer = json.loads(body)
            response_text = outer.get("response", "")
        except Exception:
            response_text = ""
        parsed_tags: list[str] = []
        if response_text:
            try:
                parsed = json.loads(response_text)
                if isinstance(parsed, dict) and isinstance(parsed.get("tags"), list):
                    parsed_tags = [str(v) for v in parsed["tags"]]
                elif isinstance(parsed, list):
                    parsed_tags = [str(v) for v in parsed]
            except Exception:
                parsed_tags = [p.strip() for p in re.split(r"[,\n]", response_text) if p.strip()]
        max_tags = max(1, settings.ollama_max_tags)
        for raw_tag in parsed_tags:
            cleaned = sanitize_tag(raw_tag)
            if cleaned and cleaned not in seen:
                seen.add(cleaned)
                tags.append(cleaned)
            if len(tags) >= max_tags:
                break
        return tags


    def maybe_set_ai_tags(db, recording, transcript, ollama_budget, hamdb_budget=None):
        if recording.ai_tags:
            return
        if not transcript or is_no_speech_transcript(transcript):
            return
        if ollama_budget["remaining"] <= 0 and not settings.ollama_enabled:
            return
        seed_tags: list[str] = []
        if recording.repeater_id:
            from .repeater import repeater_tags as get_repeater_tags
            from ..models import Repeater
            rptr = db.query(Repeater).filter(Repeater.id == recording.repeater_id).first()
            if rptr:
                seed_tags = get_repeater_tags(rptr)
        if not settings.ollama_enabled:
            if seed_tags:
                recording.ai_tags = dump_ai_tags(seed_tags)
            return
        if ollama_budget["remaining"] <= 0:
            if seed_tags:
                recording.ai_tags = dump_ai_tags(seed_tags)
            return
        ollama_budget["remaining"] -= 1
        repeater_info = None
        if recording.repeater_id:
            from ..models import Repeater
            rptr = db.query(Repeater).filter(Repeater.id == recording.repeater_id).first()
            if rptr:
                repeater_info = f"{rptr.callsign} ({rptr.location}, {rptr.state})"
                if rptr.pl_tone:
                    repeater_info += f" PL {rptr.pl_tone:.1f}"
        operator_context = None
        callsigns = extract_callsign_tags(transcript)
        if callsigns and hamdb_budget is not None:
            info_map = lookup_callsigns(db, callsigns, hamdb_budget)
            operator_context = callsign_context_str(info_map)
        tags = generate_ollama_tags(
            transcript,
            frequency_label=recording.frequency_label,
            repeater_info=repeater_info,
            operator_context=operator_context,
        )
        merged = list(dict.fromkeys(seed_tags + tags))
        serialized = dump_ai_tags(merged)
        if serialized:
            recording.ai_tags = serialized


    def should_auto_delete_no_speech(transcript: str | None) -> bool:
        return settings.auto_delete_no_speech and is_no_speech_transcript(transcript)


    def maybe_set_frequency_metadata(db, recording):
        if recording.frequency_hz is None:
            return
        if recording.frequency_label is None:
            # Check user-defined DB labels first, then fall back to static known_freqs
            try:
                from ..models import FrequencyLabel
                db_label = db.execute(
                    text(
                        "SELECT label FROM frequency_labels"
                        " WHERE ABS(frequency_hz - :f) <= COALESCE(bandwidth_hz, 5000)"
                        " ORDER BY ABS(frequency_hz - :f) LIMIT 1"
                    ),
                    {"f": recording.frequency_hz},
                ).first()
                if db_label:
                    recording.frequency_label = db_label[0]
            except Exception:
                pass
            if recording.frequency_label is None:
                label = lookup_known_freq(recording.frequency_hz)
                if label:
                    recording.frequency_label = label
        if recording.repeater_id is None:
            rptr = lookup_repeater(db, recording.frequency_hz)
            if rptr:
                recording.repeater_id = rptr.id
                recording.frequency_label = repeater_label(rptr)


    def index_directory(mode: str, audio_dir: str, text_dir: str, ollama_budget: dict, hamdb_budget: dict | None = None):
        db = SessionLocal()
        try:
            ensure_recordings_schema(db)
            audio_path = Path(audio_dir)
            if not audio_path.exists():
                return
            for wav_file in audio_path.glob("*.wav"):
                filename = wav_file.name
                text_path = os.path.join(text_dir, filename.replace(".wav", ".txt"))
                existing = db.query(Recording).filter(Recording.filename == filename).first()
                if existing:
                    if should_auto_delete_no_speech(existing.transcript):
                        safe_unlink(existing.audio_path)
                        safe_unlink(existing.text_path)
                        safe_unlink(existing.waveform_cached)
                        safe_unlink(existing.spectrogram_cached)
                        db.delete(existing)
                        db.commit()
                        print(f"Auto-deleted no-speech recording: {filename}")
                        continue
                    if existing.transcript is None:
                        transcript = read_transcript(text_path)
                        if transcript:
                            if should_auto_delete_no_speech(transcript):
                                safe_unlink(str(wav_file))
                                safe_unlink(text_path)
                                db.delete(existing)
                                db.commit()
                                print(f"Auto-deleted no-speech recording: {filename}")
                                continue
                            existing.text_path = text_path
                            existing.transcript = transcript
                            update_search_vector(db, existing.id, transcript)
                    maybe_set_frequency_metadata(db, existing)
                    maybe_set_ai_tags(db, existing, existing.transcript, ollama_budget, hamdb_budget)
                    check_alerts(existing, db)
                    db.commit()
                    continue
                if mode == "voice":
                    metadata = parse_voice_filename(filename)
                else:
                    metadata = parse_cw_filename(filename)
                timestamp = metadata.get("timestamp")
                if timestamp is None:
                    try:
                        timestamp = datetime.fromtimestamp(wav_file.stat().st_mtime)
                    except Exception:
                        timestamp = None
                duration = get_audio_duration(str(wav_file))
                transcript = read_transcript(text_path)
                if should_auto_delete_no_speech(transcript):
                    safe_unlink(str(wav_file))
                    safe_unlink(text_path)
                    print(f"Auto-deleted no-speech recording: {filename}")
                    continue
                recording = Recording(
                    filename=filename,
                    mode=mode,
                    frequency_hz=metadata.get("frequency_hz"),
                    timestamp=timestamp,
                    duration_seconds=duration,
                    audio_path=str(wav_file),
                    text_path=text_path if transcript else None,
                    transcript=transcript,
                )
                maybe_set_frequency_metadata(db, recording)
                maybe_set_ai_tags(db, recording, transcript, ollama_budget, hamdb_budget)
                check_alerts(recording, db)
                db.add(recording)
                db.commit()
                if transcript:
                    update_search_vector(db, recording.id, transcript)
                print(f"Indexed: {filename}")
        except Exception as e:
            print(f"Indexer error: {e}")
            db.rollback()
        finally:
            db.close()


    def index_aprs_directory(text_dir: str):
        """Index APRS packets from text-only files (audio deleted after decode)."""
        db = SessionLocal()
        try:
            ensure_recordings_schema(db)
            text_path = Path(text_dir)
            if not text_path.exists():
                return
            for txt_file in text_path.glob("*.txt"):
                wav_name = txt_file.name.replace(".txt", ".wav")
                existing = db.query(Recording).filter(Recording.filename == wav_name).first()
                if existing:
                    continue
                metadata = parse_voice_filename(wav_name)
                timestamp = metadata.get("timestamp")
                if timestamp is None:
                    try:
                        timestamp = datetime.fromtimestamp(txt_file.stat().st_mtime)
                    except Exception:
                        timestamp = None
                transcript = read_transcript(str(txt_file))
                if not transcript:
                    continue
                recording = Recording(
                    filename=wav_name,
                    mode="aprs",
                    frequency_hz=metadata.get("frequency_hz"),
                    timestamp=timestamp,
                    duration_seconds=None,
                    audio_path=None,
                    text_path=str(txt_file),
                    transcript=transcript,
                )
                maybe_set_frequency_metadata(db, recording)
                # Tag with callsigns from the packet (no Ollama for structured APRS data)
                callsign_tags = extract_callsign_tags(transcript)
                if callsign_tags:
                    recording.ai_tags = dump_ai_tags(callsign_tags)
                check_alerts(recording, db)
                db.add(recording)
                db.commit()
                if transcript:
                    update_search_vector(db, recording.id, transcript)
                print(f"Indexed APRS: {txt_file.name}")
        except Exception as e:
            print(f"APRS indexer error: {e}")
            db.rollback()
        finally:
            db.close()


    async def run_indexer():
        while True:
            try:
                await asyncio.to_thread(prune_missing_recordings)

                ollama_budget = {"remaining": max(0, settings.ollama_max_per_cycle)}
                hamdb_budget = {"remaining": max(0, settings.hamdb_max_per_cycle)}

                await asyncio.to_thread(
                    index_directory,
                    mode="voice",
                    audio_dir=os.path.join(settings.audio_base_path, "voice"),
                    text_dir=os.path.join(settings.text_base_path, "voice"),
                    ollama_budget=ollama_budget,
                    hamdb_budget=hamdb_budget,
                )

                await asyncio.to_thread(
                    index_directory,
                    mode="cw",
                    audio_dir=os.path.join(settings.audio_base_path, "cw"),
                    text_dir=os.path.join(settings.text_base_path, "cw"),
                    ollama_budget=ollama_budget,
                    hamdb_budget=hamdb_budget,
                )

                await asyncio.to_thread(
                    index_aprs_directory,
                    text_dir=os.path.join(settings.text_base_path, "aprs"),
                )

            except Exception as e:
                print(f"Indexer cycle error: {e}")

            await asyncio.sleep(30)

kind: ConfigMap
metadata:
  creationTimestamp: null
  name: sdr-viewer-api-overrides
  namespace: sdr-research
