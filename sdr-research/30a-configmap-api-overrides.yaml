apiVersion: v1
kind: ConfigMap
metadata:
  name: sdr-viewer-api-overrides
  namespace: sdr-research
data:
  audio.py: |
    import numpy as np
    import librosa
    import librosa.display
    import matplotlib
    matplotlib.use("Agg")
    import matplotlib.pyplot as plt


    def generate_waveform_peaks(audio_path: str, num_peaks: int = 1000) -> dict:
        """Generate waveform peaks for visualization.

        Returns a dict with:
        - peaks: list of [min, max] values per segment
        - duration: total duration in seconds
        - sample_rate: original sample rate
        """
        # Load audio
        y, sr = librosa.load(audio_path, sr=None, mono=True)
        duration = len(y) / sr

        # Calculate samples per peak
        samples_per_peak = max(1, len(y) // num_peaks)

        peaks = []
        for i in range(0, len(y), samples_per_peak):
            segment = y[i : i + samples_per_peak]
            if len(segment) > 0:
                peaks.append([float(np.min(segment)), float(np.max(segment))])

        return {
            "peaks": peaks,
            "duration": duration,
            "sample_rate": sr,
            "num_samples": len(y),
        }


    def generate_spectrogram(
        audio_path: str,
        output_path: str,
        figsize: tuple = (12, 4),
        max_seconds: int = 90,
    ):
        """Generate spectrogram image and save to file."""
        # Bound decode duration to avoid huge memory/latency on very long captures.
        y, sr = librosa.load(audio_path, sr=8000, mono=True, duration=max_seconds)

        # Compute spectrogram
        hop_length = max(256, len(y) // 2000) if len(y) else 256
        D = librosa.amplitude_to_db(
            np.abs(librosa.stft(y, n_fft=512, hop_length=hop_length)),
            ref=np.max,
        )

        # Create figure
        fig, ax = plt.subplots(figsize=figsize)
        img = librosa.display.specshow(D, sr=sr, x_axis="time", y_axis="hz", ax=ax, cmap="magma")
        ax.set_xlabel("Time (s)")
        ax.set_ylabel("Frequency (Hz)")
        fig.colorbar(img, ax=ax, format="%+2.0f dB")

        # Save
        plt.tight_layout()
        plt.savefig(output_path, dpi=100, bbox_inches="tight", pad_inches=0.1)
        plt.close(fig)


    def get_time_segments(transcript: str, duration: float) -> list:
        """Estimate time segments for transcript words.

        Since Whisper doesn't always provide word-level timestamps in our setup,
        we estimate based on character position and total duration.
        """
        if not transcript:
            return []

        words = transcript.split()
        total_chars = len(transcript)
        if total_chars == 0:
            return []

        segments = []
        char_pos = 0
        for word in words:
            start_time = (char_pos / total_chars) * duration
            char_pos += len(word) + 1  # +1 for space
            end_time = (char_pos / total_chars) * duration
            segments.append({
                "word": word,
                "start": start_time,
                "end": end_time,
            })

        return segments
